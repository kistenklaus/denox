#version 460

#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_16bit_storage                   : require
#extension GL_KHR_shader_subgroup_basic                  : enable
#extension GL_KHR_cooperative_matrix : require

#pragma use_vulkan_memory_model

layout(constant_id = 0) const uint C = 8;
layout(constant_id = 1) const uint K = 8;

const uint S = 3; // kernel width
const uint R = 3; // kernel height

const uint TILE_W = 16;
const uint TILE_H = 8;

const uint PADDING_W = 1;
const uint PADDING_H = 1;

const uint SUBGROUP_SIZE = 32;
const uint WORKGROUP_SIZE = 256;
const uint SUBGROUP_COUNT = (WORKGROUP_SIZE / SUBGROUP_SIZE);
const uint INPUT_ROWS_PER_SUBGROUP = TILE_W * TILE_H / SUBGROUP_COUNT;

// cooperative matrix size NxKxM
const uint COOPMAT_N = 16;
const uint COOPMAT_K = 8;
const uint COOPMAT_M = 8;

layout(set = 0, binding = 0) readonly buffer input_tensor {
    uvec4 inputTensor[]; // CHW8 (f16)
};

layout(set = 0, binding = 1) writeonly buffer output_tensor {
    float16_t outputTensor[]; // CHW8 (f16)
};

layout(set = 0, binding = 2) readonly buffer weight_tensor {
    uvec4 weightTensor[]; // KSRC
};

layout(push_constant) uniform PushConstant {
    uint Input_W;
    uint Input_H;
} pc;

const uint PIPELINE_DEPTH = 2;
shared f16vec2 sh_A[PIPELINE_DEPTH][TILE_W * TILE_H * COOPMAT_K / 2];
shared f16vec2 sh_B[PIPELINE_DEPTH][COOPMAT_K * K / 2];

void loadA(uint stage, uint row, uvec2 pixel, uint cSlice)
{
    if (gl_SubgroupInvocationID >= 16) return;

    bool inBounds = pixel.x < pc.Input_W && pixel.y < pc.Input_H; // abuse uint underflow.

    if (inBounds) {
        uint gIndex = ((pixel.y * pc.Input_W + pixel.x) * COOPMAT_K + cSlice * (pc.Input_W * pc.Input_H * COOPMAT_K)) / 8;
        uvec4 v = inputTensor[gIndex];
        sh_A[stage][row * COOPMAT_K / 2 + 0u] = unpackFloat2x16(uint32_t(v.x));
        sh_A[stage][row * COOPMAT_K / 2 + 1u] = unpackFloat2x16(uint32_t(v.y));
        sh_A[stage][row * COOPMAT_K / 2 + 2u] = unpackFloat2x16(uint32_t(v.z));
        sh_A[stage][row * COOPMAT_K / 2 + 3u] = unpackFloat2x16(uint32_t(v.w));
    } else {
        sh_A[stage][row * COOPMAT_K / 2 + 0u] = unpackFloat2x16(0);
        sh_A[stage][row * COOPMAT_K / 2 + 1u] = unpackFloat2x16(0);
        sh_A[stage][row * COOPMAT_K / 2 + 2u] = unpackFloat2x16(0);
        sh_A[stage][row * COOPMAT_K / 2 + 3u] = unpackFloat2x16(0);
    }
}

void loadB(uint stage, uint k) {
    const uint offset = k * K;
    for (uint o = gl_LocalInvocationID.x; o < COOPMAT_K * K / 8; o += WORKGROUP_SIZE) {
        uvec4 v = weightTensor[o + offset];
        sh_B[stage][o * 4 + 0] = unpackFloat2x16(uint32_t(v.x));
        sh_B[stage][o * 4 + 1] = unpackFloat2x16(uint32_t(v.y));
        sh_B[stage][o * 4 + 2] = unpackFloat2x16(uint32_t(v.z));
        sh_B[stage][o * 4 + 3] = unpackFloat2x16(uint32_t(v.w));
    }
}

layout(local_size_x = WORKGROUP_SIZE, local_size_y = 1, local_size_z = 1) in;
void main(void) {
    const uint wgPerRow = pc.Input_W / TILE_W;
    const uvec2 wg2d = uvec2(gl_WorkGroupID.x % wgPerRow, gl_WorkGroupID.x / wgPerRow);
    const uvec2 wpixel = wg2d * uvec2(TILE_W, TILE_H);

    const uint sid = gl_SubgroupID;
    const uint iid = gl_SubgroupInvocationID;

    const uint row = (sid << 4) + (iid & 0xF); // sid * 16 + iid % 16.
    const uvec2 lpixel = wpixel + uvec2(iid & 0xF, sid) - uvec2(PADDING_W, PADDING_H);

    const uint cFragCount = K / COOPMAT_M;
    coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_M, gl_MatrixUseAccumulator > cFrag[cFragCount];

    for (uint n = 0; n < cFragCount; ++n) {
        cFrag[n] = coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_M, gl_MatrixUseAccumulator > (float16_t(0));
    }

    const uint aFragOffset = (sid << 4) * (COOPMAT_K / 2);

    uint stage = 0u;

    loadA(stage, row, lpixel, 0);
    loadB(stage, 0);

    uint c = 1;
    uint k = 1;
    for (uint r = 0; r < R; ++r) {
        for (uint s = 0; s < S; ++s) {
            uvec2 pixel = lpixel + uvec2(s, r);
            for (; c < (C / COOPMAT_K); ++c) {
                loadA(stage ^ 1, row, pixel, c);
                loadB(stage ^ 1, k++);

                memoryBarrierShared();
                barrier();

                coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_K, gl_MatrixUseA > aFrag;
                coopMatLoad(aFrag, sh_A[stage], aFragOffset, COOPMAT_K / 2,
                    gl_CooperativeMatrixLayoutRowMajor);

                for (uint o = 0, n = 0; n < cFragCount; o += (COOPMAT_M * COOPMAT_K / 2), ++n) {
                    coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_K, COOPMAT_M, gl_MatrixUseB > bFrag;

                    coopMatLoad(bFrag, sh_B[stage], o, (COOPMAT_K / 2),
                        gl_CooperativeMatrixLayoutColumnMajor);

                    cFrag[n] = coopMatMulAdd(aFrag, bFrag, cFrag[n]);
                }
                stage ^= 1;
            }
            c = 0;
        }
    }

    coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_K, gl_MatrixUseA > aFrag;
    coopMatLoad(aFrag, sh_A[stage], aFragOffset, COOPMAT_K / 2,
        gl_CooperativeMatrixLayoutRowMajor);

    for (uint o = 0, n = 0; n < cFragCount; o += (COOPMAT_M * COOPMAT_K / 2), ++n) {
        coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_K, COOPMAT_M, gl_MatrixUseB > bFrag;
        coopMatLoad(bFrag, sh_B[stage], o, (COOPMAT_K / 2),
            gl_CooperativeMatrixLayoutColumnMajor);
        cFrag[n] = coopMatMulAdd(aFrag, bFrag, cFrag[n]);
    }

    uint offset = wpixel.x * COOPMAT_M + wpixel.y * pc.Input_W * COOPMAT_M + gl_SubgroupID * COOPMAT_M * pc.Input_W;
    uint stride = pc.Input_W * pc.Input_H * COOPMAT_M;

    for (uint n = 0; n < cFragCount; ++n) {
        coopMatStore(cFrag[n], outputTensor,
            offset,
            COOPMAT_M,
            gl_CooperativeMatrixLayoutRowMajor);
        offset += stride;
    }
}
