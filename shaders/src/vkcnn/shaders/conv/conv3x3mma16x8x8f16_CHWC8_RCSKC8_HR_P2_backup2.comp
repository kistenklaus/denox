#version 460

#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_16bit_storage                   : require
#extension GL_EXT_scalar_block_layout : require
#extension GL_KHR_shader_subgroup_basic                  : enable
#extension GL_KHR_cooperative_matrix : require

#pragma use_vulkan_memory_model

layout(constant_id = 0) const uint C = 8;
layout(constant_id = 1) const uint K = 8;

const uint S = 3; // kernel width
const uint R = 3; // kernel height

const uint TILE_W = 16;
const uint TILE_H = 8;

const uint PADDING_W = 1;
const uint PADDING_H = 1;

// cooperative matrix size NxKxM
const uint COOPMAT_N = 16;
const uint COOPMAT_K = 8;
const uint COOPMAT_M = 8;

const uint FRAG_C_COUNT = K / COOPMAT_M;

layout(set = 0, binding = 0) readonly buffer input_tensor {
    uvec4 inputTensor[]; // CHW8 (f16)
};

layout(std430, set = 0, binding = 1, scalar) writeonly buffer output_tensor {
    uvec4 outputTensor[]; // CHW8 (f16)
};

layout(set = 0, binding = 2) readonly buffer weight_tensor {
    uvec4 weightTensor[]; // KSRC
};

layout(push_constant) uniform PushConstant {
    uint Input_W;
    uint Input_H;
} pc;

const uint PIPELINE_DEPTH = 2;
shared uvec4 sh_A[PIPELINE_DEPTH][((COOPMAT_N + (S - 1)) * TILE_H * COOPMAT_K) / 8];
shared uvec4 sh_B[PIPELINE_DEPTH][(COOPMAT_K * K * S) / 8];

shared uvec4 sh_C[(COOPMAT_N * COOPMAT_M * TILE_H)];

coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_M, gl_MatrixUseAccumulator > cFrag[FRAG_C_COUNT];

void loadA(uint stage, uint ix, uint iy, uint r, uint iOffset, uint row) {
    if (gl_SubgroupInvocationID < (TILE_W + (S - 1))) {
        const bool inBounds = ix < pc.Input_W && iy < pc.Input_H;
        if (inBounds) {
            uint gIndex = ((r * pc.Input_W * COOPMAT_K / 8) + iOffset);
            uvec4 v = inputTensor[gIndex];
            sh_A[stage][row * COOPMAT_K / 8] = v;
        } else {
            sh_A[stage][row * COOPMAT_K / 8] = uvec4(0);
        }
    }
}

void loadB(uint stage, uint r, uint wOffset) {
    const uint offset = ((r * C * S * K) / 8 + wOffset);
    for (uint o = gl_LocalInvocationID.x; o < (COOPMAT_K * K / 8) * 3; o += gl_WorkGroupSize.x) {
        uvec4 v = weightTensor[o + offset];
        sh_B[stage][o * 4 / 4] = v;
    }
}

void consumeGEMM(uint stage) {
    uint aOffset = gl_SubgroupID * (TILE_W + (S - 1)) * COOPMAT_K / 8;
    uint bOffset = 0;
    for (uint s = 0; s < S; ++s, aOffset += COOPMAT_K / 8, bOffset += COOPMAT_K * K / 8) {
        coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_K, gl_MatrixUseA > aFrag;
        coopMatLoad(aFrag, sh_A[stage], aOffset, COOPMAT_K / 8,
            gl_CooperativeMatrixLayoutRowMajor);

        for (uint o = bOffset, n = 0; n < FRAG_C_COUNT; o += (COOPMAT_M * COOPMAT_K / 8), ++n) {
            coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_K, COOPMAT_M, gl_MatrixUseB > bFrag;
            coopMatLoad(bFrag, sh_B[stage], o, (COOPMAT_K / 8),
                gl_CooperativeMatrixLayoutColumnMajor);

            cFrag[n] = coopMatMulAdd(aFrag, bFrag, cFrag[n]);
        }
    }
}

layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;
void main(void) {
    const uint wgPerRow = (pc.Input_W + TILE_W - 1) / TILE_W;
    const uvec2 wg2d = uvec2(gl_WorkGroupID.x % wgPerRow, gl_WorkGroupID.x / wgPerRow);
    const uvec2 wpixel = wg2d * uvec2(TILE_W, TILE_H);

    for (uint n = 0; n < FRAG_C_COUNT; ++n) {
        cFrag[n] = coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_M, gl_MatrixUseAccumulator > (float16_t(0));
    }

    const uint ix = wpixel.x + gl_SubgroupInvocationID - PADDING_W;
    const uint row = gl_SubgroupID * (TILE_W + (S - 1)) + gl_SubgroupInvocationID; // subinvoc is in 0..17

    uint stage = 0u;
    uint y = wpixel.y + gl_SubgroupID - PADDING_H;
    uint iOffset = ix * COOPMAT_K / 8;
    uint wOffset = 0;

    loadA(stage, ix, y, y, iOffset, row);
    iOffset += y * pc.Input_W * COOPMAT_K / 8;
    loadB(stage, 0, wOffset);

    uint r = 1;
    for (uint c = 0; c < (C / COOPMAT_K); ++c, iOffset += pc.Input_W * pc.Input_H * COOPMAT_K / 8, wOffset += S * K * 8 / 8) {
        for (; r < R; ++r) {
            loadA(stage ^ 1, ix, y + r, r, iOffset, row);
            loadB(stage ^ 1, r, wOffset);

            memoryBarrierShared();
            barrier();

            consumeGEMM(stage);
            stage ^= 1;
        }
        r = 0;
    }

    // pipeline epilog.
    consumeGEMM(stage);

    // Writeback
    bool boundaryTile = (wpixel.x + TILE_W - 1) > pc.Input_W;
    // bool boundaryTile = true;
    if (boundaryTile) {
        uint offset = (wpixel.x * (COOPMAT_M) + (wpixel.y + gl_SubgroupID) * (pc.Input_W * COOPMAT_M)) / 8;
        uint stride = pc.Input_W * pc.Input_H * COOPMAT_M / 8;

        for (uint n = 0; n < FRAG_C_COUNT; ++n) {
            uint sharedOffet = gl_SubgroupID * COOPMAT_N * COOPMAT_M / 8;
            coopMatStore(cFrag[n], sh_C,
                sharedOffet,
                1,
                gl_CooperativeMatrixLayoutRowMajor);

            subgroupBarrier();

            if (gl_SubgroupInvocationID.x < TILE_W) {
                uvec4 v = sh_C[sharedOffet + gl_SubgroupInvocationID.x];
                if (wpixel.x + gl_SubgroupInvocationID.x < pc.Input_W) {
                    outputTensor[offset + gl_SubgroupInvocationID] = v;
                }
            }
            offset += stride;
        }
    } else {
        uint offset = wpixel.x * COOPMAT_M + (wpixel.y + gl_SubgroupID) * pc.Input_W * COOPMAT_M;
        uint stride = pc.Input_W * pc.Input_H * COOPMAT_M;
        for (uint n = 0; n < FRAG_C_COUNT; ++n) {
            coopMatStore(cFrag[n], outputTensor,
                offset / 8,
                COOPMAT_M / 8,
                gl_CooperativeMatrixLayoutRowMajor);
            offset += stride;
        }
    }
}
