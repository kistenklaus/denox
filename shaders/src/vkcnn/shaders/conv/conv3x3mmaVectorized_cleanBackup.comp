#version 460

#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_16bit_storage                   : require
#extension GL_EXT_scalar_block_layout : require
#extension GL_KHR_shader_subgroup_basic                  : enable
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_cooperative_matrix : require

#pragma use_vulkan_memory_model

layout(constant_id = 0) const uint C = 8;
layout(constant_id = 1) const uint K = 8;

layout(constant_id = 2) const uint COOPMAT_N = 16;
layout(constant_id = 3) const uint COOPMAT_K = 16;
layout(constant_id = 4) const uint COOPMAT_M = 16;


const uint SUBGROUP_SIZE = 32;
const uint WORKGROUP_SIZE = 256;

// These constants are not modifiable without
// having a to change the shader completely.
const uint PADDING_W = 1;
const uint PADDING_H = 1;
const uint S = 3; // filter width
const uint R = 3; // filter height
const uint TILE_W = 16;
const uint TILE_H = 8;

// Size of largest f16 vector, which can be writting in a single
// memory access (Not the same as a transaction).
const uint F16VEC = 8;

const uint FRAG_C_COUNT = K / COOPMAT_M;

layout(set = 0, binding = 0) readonly buffer input_tensor {
    uvec4 inputTensor[]; // CHW8/16 (f16)
};

layout(std430, set = 0, binding = 1, scalar) writeonly buffer output_tensor {
    uvec4 outputTensor[]; // CHW8/16 (f16)
};

layout(set = 0, binding = 2) readonly buffer weight_tensor {
    uvec4 filterTensor[]; // RCSKC8 (f16)
};

layout(push_constant) uniform PushConstant {
    uint Input_W;
    uint Input_H;
} pc;

const uint PIPELINE_DEPTH = 2;

// Static shared memory allocation (super ugly, but allows reuse of shared memory)
const uint SH_A_SIZE = ((COOPMAT_N + (S - 1)) * TILE_H * COOPMAT_K) / F16VEC;
const uint SH_B_SIZE = (COOPMAT_K * K * S) / F16VEC;
const uint SH_C_SIZE = COOPMAT_N * COOPMAT_M * TILE_H / F16VEC;

const uint SH_PIPE_SIZE = PIPELINE_DEPTH * SH_A_SIZE + PIPELINE_DEPTH * SH_B_SIZE;
const uint SH_WB_SIZE = SH_C_SIZE;
const uint SH_SIZE = SH_PIPE_SIZE > SH_WB_SIZE ? SH_PIPE_SIZE : SH_WB_SIZE;
const uint SH_A_OFFSET = 0;
const uint SH_B_OFFSET = PIPELINE_DEPTH * SH_A_SIZE;
const uint SH_C_OFFSET = 0;

shared uvec4 sh_mem[SH_SIZE];

const uint VEC_LANES = COOPMAT_K / F16VEC;
const uint SUBGROUP_VEC_LANES = SUBGROUP_SIZE / VEC_LANES;
const uint WORKGROUP_VEC_LANES = WORKGROUP_SIZE / VEC_LANES;

const uint PADDED_TILE_W = TILE_W + (S - 1);

coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_M, gl_MatrixUseAccumulator > cFrag[FRAG_C_COUNT];

void loadA(in uint stage,
    in uint lane,
    in uint shAOffset,
    in uint iy,
    in uint ix,
    in uint r,
    in uint cBlk,
    in uint iOffset
) {
    uint shAStageOffset = SH_A_OFFSET + stage * SH_A_SIZE + shAOffset;
    if /* constexpr */ (PADDED_TILE_W * VEC_LANES <= SUBGROUP_SIZE) {
        // NOTE: force unrolling if possible within a single iteration.
        if (gl_SubgroupInvocationID < PADDED_TILE_W * VEC_LANES) {
            const uint ixm = ix + gl_SubgroupInvocationID;
            const bool inBounds = ixm / VEC_LANES < pc.Input_W && iy + r < pc.Input_H;
            uint gIndex = iOffset + ixm;
            uvec4 v = inBounds ? inputTensor[gIndex] : uvec4(0);
            sh_mem[shAStageOffset + gl_SubgroupInvocationID] = v;
        }
    } else {
        for (uint x = gl_SubgroupInvocationID, ixm = ix + x;
            /* */ x < PADDED_TILE_W * VEC_LANES;
            /* */ x += SUBGROUP_SIZE, ixm += SUBGROUP_SIZE) {
            const bool inBounds = ixm / VEC_LANES < pc.Input_W && iy + r < pc.Input_H;
            uint gIndex = iOffset + ixm;
            uvec4 v = inBounds ? inputTensor[gIndex] : uvec4(0);
            sh_mem[shAStageOffset + x] = v;
        }
    }
}

void loadB(in uint stage, in uint lane, in uint wOffset) {
    const uint shBStageOffset = SH_B_OFFSET + stage * SH_B_SIZE;
    if /* constexpr */ (S * COOPMAT_K * K / F16VEC < WORKGROUP_SIZE) {
        if (gl_LocalInvocationID.x < (S * COOPMAT_K * K / F16VEC)) {
            sh_mem[shBStageOffset + gl_LocalInvocationID.x] = filterTensor[wOffset];
        }
    } else {
        for (uint o = wOffset, x = gl_LocalInvocationID.x;
            /* */ x < S * COOPMAT_K * K / F16VEC;
            /* */ o += WORKGROUP_VEC_LANES, x += WORKGROUP_SIZE) {
            uvec4 v = filterTensor[o];
            sh_mem[shBStageOffset + x] = v;
        }
    }
}

void consumeGEMM(in uint stage, in uint shAOffset) {
    uint aOffset = shAOffset;
    uint bOffset = 0;
    for (uint s = 0; s < S; ++s, aOffset += COOPMAT_K / F16VEC, bOffset += COOPMAT_K * K / F16VEC) {
        coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_K, gl_MatrixUseA > aFrag;

        coopMatLoad(aFrag, sh_mem, SH_A_OFFSET + stage * SH_A_SIZE + aOffset, COOPMAT_K / F16VEC,
            gl_CooperativeMatrixLayoutRowMajor);

        for (uint o = bOffset, n = 0; n < FRAG_C_COUNT; o += (COOPMAT_M * COOPMAT_K / F16VEC), ++n) {
            coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_K, COOPMAT_M, gl_MatrixUseB > bFrag;

            coopMatLoad(bFrag, sh_mem, SH_B_OFFSET + stage * SH_B_SIZE + o, COOPMAT_K / F16VEC,
                gl_CooperativeMatrixLayoutColumnMajor);

            cFrag[n] = coopMatMulAdd(aFrag, bFrag, cFrag[n]);
        }
    }
}

uvec2 wpixel() {
    const uint wgPerRow = (pc.Input_W + TILE_W - 1) / TILE_W;
    const uvec2 wg2d = uvec2(gl_WorkGroupID.x % wgPerRow, gl_WorkGroupID.x / wgPerRow);
    return wg2d * uvec2(TILE_W, TILE_H);
}

layout(local_size_x = WORKGROUP_SIZE, local_size_y = 1, local_size_z = 1) in;
void main(void) {
    // ================== SETUP ==================
    uvec2 wpixel = wpixel();

    for (uint n = 0; n < FRAG_C_COUNT; ++n) {
        cFrag[n] = coopmat < float16_t, gl_ScopeSubgroup, COOPMAT_N, COOPMAT_M, gl_MatrixUseAccumulator > (float16_t(0));
    }

    const uint y = wpixel.y + gl_SubgroupID;
    const uint iy = y - PADDING_H;

    const uint ix = (wpixel.x - PADDING_W) * VEC_LANES;

    const uint lane = gl_SubgroupInvocationID & (VEC_LANES - 1);
    const uint shAOffset = (gl_SubgroupID * (COOPMAT_K * (COOPMAT_N + (S - 1)))) / F16VEC;

    uint stage = 0u;
    uint iOffsetBase = ((wpixel.y - PADDING_H) + gl_SubgroupID) * ((pc.Input_W * COOPMAT_K) / F16VEC);
    uint wOffset = lane * K * S + gl_LocalInvocationID.x / VEC_LANES;

    // ============= PIPE-PROLOG ==============
    {
        loadA(stage, lane, shAOffset, iy, ix, 0, 0, iOffsetBase);
    }

    {
        loadB(stage, lane, wOffset);
    }
    memoryBarrierShared();
    barrier();
    wOffset += S * K * F16VEC * VEC_LANES / F16VEC;

    uint c = 1;

    uint iOffset = iOffsetBase + c * pc.Input_H * pc.Input_W * COOPMAT_K / F16VEC;
    for (uint r = 0; r < R; ++r) {
        for (; c < (C / COOPMAT_K); ++c, wOffset += S * K * F16VEC * VEC_LANES / F16VEC, iOffset += pc.Input_H * pc.Input_W * COOPMAT_K / F16VEC) {
            uint nextStage = (stage + 1) % PIPELINE_DEPTH;
            // ============= PIPE-STAGE-LOADING ===============
            if (y < pc.Input_H) {
                loadA(nextStage, lane, shAOffset, iy, ix, r, c, iOffset);
            }
            {
                loadB(nextStage, lane, wOffset);
            }
            // ============ PIPE-STAGE-CONSUMING ==============

            if (y < pc.Input_H) {
                consumeGEMM(stage, shAOffset);
            }

            memoryBarrierShared();
            barrier();
            stage = nextStage;
        }
        c = 0;
        iOffsetBase += (pc.Input_W * COOPMAT_K) / F16VEC;
        iOffset = iOffsetBase;
    }

    if (y >= pc.Input_H) {
        return;
    }

    // ================= PIPE-EPILOG ==================
    consumeGEMM(stage, shAOffset);

    barrier();

    // ================= WRITE-BACK ===================
    bool boundaryTile = (wpixel.x + TILE_W) > pc.Input_W;
    if (boundaryTile) {
        uint offset = (wpixel.x * COOPMAT_M + (wpixel.y + gl_SubgroupID) * (pc.Input_W * COOPMAT_M)) / F16VEC;
        uint stride = pc.Input_W * pc.Input_H * COOPMAT_M / F16VEC;

        for (uint n = 0; n < FRAG_C_COUNT; ++n) {
            uint sharedOffet = gl_SubgroupID * COOPMAT_N * COOPMAT_M / F16VEC;
            coopMatStore(cFrag[n], sh_mem,
                SH_C_OFFSET + sharedOffet,
                COOPMAT_M / F16VEC,
                gl_CooperativeMatrixLayoutRowMajor);

            subgroupMemoryBarrierShared();
            subgroupBarrier();

            // CHWC16 (16x16x16)
            if (gl_SubgroupInvocationID < (COOPMAT_N * COOPMAT_M / F16VEC)) {
                uvec4 v = sh_mem[SH_C_OFFSET + sharedOffet + gl_SubgroupInvocationID];
                uint x = gl_SubgroupInvocationID / (COOPMAT_M / F16VEC);
                if (wpixel.x + x < pc.Input_W) {
                    outputTensor[offset + gl_SubgroupInvocationID] = v;
                }
            }
            offset += stride;
        }
    } else {
        uint offset = (wpixel.x * COOPMAT_M + (wpixel.y + gl_SubgroupID) * pc.Input_W * COOPMAT_M) / F16VEC;
        uint stride = (pc.Input_H * pc.Input_W * COOPMAT_M) / F16VEC;
        for (uint n = 0; n < FRAG_C_COUNT; ++n) {
            coopMatStore(cFrag[n], outputTensor,
                offset,
                COOPMAT_M / F16VEC,
                gl_CooperativeMatrixLayoutRowMajor);
            offset += stride;
        }
    }
}
