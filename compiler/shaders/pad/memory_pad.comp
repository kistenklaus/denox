#version 460

#extension GL_EXT_control_flow_attributes : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_explicit_arithmetic_types_int16 : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_16bit_storage                   : require

layout(set = 0, binding = 0, std430) readonly buffer input_buf {
    istype input_tensor_blob[];
};

layout(set = 0, binding = 1, std430) writeonly buffer output_buf {
    ostype output_tensor_blob[];
};

layout(push_constant) uniform PushConstant {
    uint OUT_W;
    uint OUT_H;
    uint PAD_LEFT;
    uint PAD_RIGHT;
    uint PAD_TOP;
    uint PAD_BOTTOM;
} pc;

const uint TILE_C = WG_C * INVOC_C;
const uint TILE_H = WG_H * INVOC_H;
const uint TILE_W = WG_W * INVOC_W;

layout(local_size_x = WG_C, local_size_y = WG_W, local_size_z = WG_H) in;
void main() {
    #if defined(IN_LAYOUT_HWC) && defined(OUT_LAYOUT_HWC)
    // => istype: uint16_t, ostype: uint16_t, atype: float16_t

    // NOTE: It might not be immediatly obvious why a memcpy kernel, does have
    // to use small 2 byte accesses. The reason is that if we
    // cannot make any assumptions about C, the total amount of bytes
    // in a HWC tensor, could not be divisible by let's say 16, we can only
    // guarantee that the HWC tensor size is divisible by atype, which is 2 byte.

    const uint wgc0 = gl_WorkGroupID.x * TILE_C;
    const uint wgw0 = gl_WorkGroupID.y * TILE_W;
    const uint wgh0 = gl_WorkGroupID.z * TILE_H;

    const uint c0 = wgc0 + gl_LocalInvocationID.x;
    const uint w0 = wgw0 + gl_LocalInvocationID.y;
    const uint h0 = wgh0 + gl_LocalInvocationID.z;

    const uint OUT_W = pc.OUT_W;
    const uint OUT_H = pc.OUT_H;
    const uint PAD_LEFT = pc.PAD_LEFT;
    const uint PAD_RIGHT = pc.PAD_RIGHT;
    const uint PAD_TOP = pc.PAD_TOP;
    const uint PAD_BOTTOM = pc.PAD_BOTTOM;

    const uint IN_W = OUT_W - PAD_LEFT - PAD_RIGHT;
    const uint IN_H = OUT_H - PAD_TOP - PAD_BOTTOM;

    #pragma unroll
    for (uint zh = 0; zh < INVOC_H; ++zh) {
        #pragma unroll
        for (uint zw = 0; zw < INVOC_W; ++zw) {
            #pragma unroll
            for (uint zc = 0; zc < INVOC_C; ++zc) {
                const uint c = c0 + zc * WG_C;
                // output coords.
                const uint ho = h0 + zh * WG_H;
                const uint wo = w0 + zw * WG_W;
                // const uint wo = 0;

                const uint hi = uint(clamp(int(ho) - int(PAD_TOP), int(0), int(IN_H - 1)));
                const uint wi = uint(clamp(int(wo) - int(PAD_LEFT), int(0), int(IN_W - 1)));

                if (ho < OUT_H && wo < OUT_W && c < CH) {
                    output_tensor_blob[ho * OUT_W * CH + wo * CH + c] =
                        // float16BitsToUint16(float16_t(OUT_H));
                        input_tensor_blob[hi * IN_W * CH + wi * CH + c];
                }
            }
        }
    }

    #elif defined(IN_LAYOUT_HWC8) && defined(OUT_LAYOUT_HWC8)
    const uint TILE_C8 = TILE_C / 8;
    const uint INVOC_C8 = INVOC_C / 8;
    const uint CH8 = CH / 8;

    const uint wgc08 = gl_WorkGroupID.x * TILE_C8;
    const uint wgw0 = gl_WorkGroupID.y * TILE_W;
    const uint wgh0 = gl_WorkGroupID.z * TILE_H;

    const uint c08 = wgc08 + gl_LocalInvocationID.x;
    const uint w0 = wgw0 + gl_LocalInvocationID.y;
    const uint h0 = wgh0 + gl_LocalInvocationID.z;

    const uint OUT_W = pc.OUT_W;
    const uint OUT_H = pc.OUT_H;
    const uint PAD_LEFT = pc.PAD_LEFT;
    const uint PAD_RIGHT = pc.PAD_RIGHT;
    const uint PAD_TOP = pc.PAD_TOP;
    const uint PAD_BOTTOM = pc.PAD_BOTTOM;

    const uint IN_W = OUT_W - PAD_LEFT - PAD_RIGHT;
    const uint IN_H = OUT_H - PAD_TOP - PAD_BOTTOM;

    const uint16_t test = float16BitsToUint16(float16_t(1));
    const uint test2 = packUint2x16(u16vec2(test, test));
    uvec4 xxx = uvec4(test2, test2, test2, test2);

    #pragma unroll
    for (uint zh = 0; zh < INVOC_H; ++zh) {
        #pragma unroll
        for (uint zw = 0; zw < INVOC_W; ++zw) {
            #pragma unroll
            for (uint zc8 = 0; zc8 < INVOC_C8; ++zc8) {
                const uint ho = h0 + zh * WG_H;
                const uint wo = w0 + zw * WG_W;

                const uint hi = uint(clamp(int(ho) - int(PAD_TOP), int(0), int(IN_H - 1)));
                const uint wi = uint(clamp(int(wo) - int(PAD_LEFT), int(0), int(IN_W - 1)));

                const uint c8 = c08 + zc8 * WG_C;
                if (ho < OUT_H && wo < OUT_W && c8 < CH8) {
                    output_tensor_blob[ho * OUT_W * CH8 + wo * CH8 + c8] =
                        input_tensor_blob[hi * IN_W * CH8 + wi * CH8 + c8];
                }
            }
        }
    }

    #else
    NOT_IMPLEMENTED;
    #endif
}
