#version 460

#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_explicit_arithmetic_types_int16 : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_16bit_storage                   : require
#extension GL_EXT_scalar_block_layout : require
#extension GL_KHR_shader_subgroup_basic                  : enable
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_shuffle_relative : require
#extension GL_EXT_shader_subgroup_extended_types_int16 : require

#pragma use_vulkan_memory_model

// basically how we plan to map this.

// Vectorized memory storage type.
// NOTE: After loading from global memory we can always vectorize.
// For example for register to shared memory copy or coopMatLoad from shared memory.
#define vstype uvec4
#define VSTYPE_SIZE 16

layout(set = INPUT_SET, binding = INPUT_BINDING, std430) readonly buffer input_buf {
  istype input_tensor_blob[];
};

layout(set = OUTPUT_SET, binding = OUTPUT_BINDING, std430) writeonly buffer output_buf {
  ostype output_tensor_blob[];
};

layout(set = FILTER_SET, binding = FILTER_BINDING, std430) readonly buffer filter_buf {
  fstype filter_blob[];
};

#ifdef USE_BIAS
layout(set = BIAS_SET, binding = BIAS_BINDING, std430) readonly buffer bias_buf {
  // NOTE: The bias blob is required to be aligned to 16byte and
  // padded to a multiple of INVOC_N.
  // This allows us to skip shared memory when loading the bias.
  vstype bias_blob[];
};
#endif

layout(push_constant) uniform PushConstant {
  uint IN_W;
  uint IN_H;
} pc;

const uint WG_SIZE = SG_SIZE * SG_COUNT;

const uint SG_TILE_X = INVOC_M;
const uint SG_TILE_Y = SG_M;

const uint WG_TILE_X = SG_TILE_X;
const uint WG_TILE_Y = SG_TILE_Y * WG_M;
const uint WG_TILE_N = INVOC_N * SG_N * WG_N;

const uint WG_TILE_N8 = (WG_TILE_N + 7u) / 8;
const uint RSC8 = (KERNEL_X * KERNEL_Y) * ((((IN_CH + 7u) & ~7u) / 8u));
// const uint OUT_CH8 = (((OUT_CH + 7u) & ~7u) / 8u);
const uint OUT_CH8 = ((OUT_CH + 7u) / 8);
const uint INVOC_M8 = INVOC_M / (VSTYPE_SIZE / ATYPE_SIZE);
const uint INVOC_N8 = INVOC_N / (VSTYPE_SIZE / ATYPE_SIZE);
const uint INVOC_K8 = INVOC_K / (VSTYPE_SIZE / ATYPE_SIZE);

const uint PIPELINE_DEPTH = 2;

// YXRSC8 <- layout
const uint SH_A_SIZE = (SG_M * WG_M) * INVOC_M * SG_K * INVOC_K8;
shared vstype sh_a[PIPELINE_DEPTH][SH_A_SIZE];

// RSC8K8 <- layout
const uint SH_B_SIZE = WG_TILE_N8 * SG_K * INVOC_K;
shared vstype sh_b[PIPELINE_DEPTH][SH_B_SIZE];

// x : channel output tile.
// y : x output / input tile.
// z : y output / input tile.
layout(local_size_x = SG_SIZE, local_size_y = SG_COUNT, local_size_z = 1) in;
void main() {
  const uvec3 wgi = gl_WorkGroupID.xyz;
  const uint sgi = gl_SubgroupID;

  // workgroup input tile (x,y) offset.
  const uvec2 wgxy = uvec2(wgi.y * WG_TILE_X, wgi.z * WG_TILE_Y);
  const uint wgni = WG_TILE_N * wgi.x;

  const uint sgmi = (sgi / WG_N);
  const uint sgni = sgi % WG_N;

  // Input tile:
  //
  //       +-INVOC_M-+  -
  //       | A B C D |  |
  //       | W X Y Z |  (SG_M x WG_M) = WG_TILE_Y
  //       |    :    |  |
  //       |    :    |  |
  //       +---------+  -
  //  covering INVOC_N * WG_N many output channels.
  //
  // sum[zn][zm][zc8] layout.
  //
  // Letters denote pixel positions and numbers denote the channel
  //
  //                       <--------WG_N--------->
  //               |-------SG_N-----|
  //
  //      -        +INVOC_N-+--------+-------+---...
  //      |        |A0A1A2A3|A4A5A6A7|  ...  |
  //  ^   | INVOC_M|B0B1B2B3|B4B5B6B7|  ...  |
  //  |   |        |C0C1C2C3|C4C5C6C7|  ...  |
  //  |   |        |D0D1D2D3|D4D5D6D7|  ...  |
  //  |  SG_M      +-------SG0-------+------SG1--...
  //  |   |        |W0W1W2W3|W4W5W6W7|  ...  |
  //  |   |        |X0X1X2X3|X4X5X6X7|  ...  |
  //  |   |        |Y0Y1Y2Y3|Y4Y5Y6Y7|  ...  |
  //  |   |        |Z0Z1Z2Z3|Z4Z5Z6Z7|  ...  |
  // WG_M -        +--------+--------+-------+--...
  //  |            |   :    |   :    |       |
  //  |            |   :    |   :    |       |
  //  |            |   :    |   :    |       |
  //  |            +-------SG2-------+------SG3--...
  //  v            |        |        |       |
  //               :        :        :       :

  // ((INVOV_N * INVOC_M) div SG_SIZE) div 8  (div is a ceiling division)
  const uint VECS_PER_INVOC = (INVOC_N * INVOC_M + SG_SIZE * (VSTYPE_SIZE / ATYPE_SIZE) - 1) / (SG_SIZE * (VSTYPE_SIZE / ATYPE_SIZE));
  vstype sum[SG_N][SG_M][VECS_PER_INVOC];

  #pragma begin_block(load_bias);
  for (uint zn = 0; zn < SG_N; ++zn) {
    for (uint zm = 0; zm < SG_M; ++zm) {
      for (uint i = 0; i < VECS_PER_INVOC; ++i) {
        sum[zn][zm][i] = vstype(0);
      }
    }
  }
  #ifdef USE_BIAS
  {
    const uint OUT_CH_PADDED8 = ((OUT_CH + INVOC_N - 1) / INVOC_N) * INVOC_N8;
    for (uint zn = 0; zn < SG_N; ++zn) {
      // base channel in *vectors* for this (wg,sg,zn) tile
      const uint zc8 =
        (wgni + (sgni * SG_N + zn) * INVOC_N) / (VSTYPE_SIZE / ATYPE_SIZE);

      for (uint idx = gl_SubgroupInvocationID; idx < (INVOC_M * INVOC_N8); idx += SG_SIZE) {
        const uint i = idx / SG_SIZE;
        const uint n8 = idx % INVOC_N8;

        const uint ch8 = zc8 + n8;
        if (ch8 < OUT_CH_PADDED8) {
          vstype b = bias_blob[ch8];
          for (uint zm = 0; zm < SG_M; ++zm) {
            sum[zn][zm][i] = b;
          }
        }
      }
    }
  }
  #endif
  #pragma end_block; // load_bias

  uint kk = 0;
  const uint KK = (RSC8 + (SG_K * INVOC_K8) - 1u) / (SG_K * INVOC_K8);
  uint pp = 0;
  uint ppx = 0;

  #pragma begin_block(prefetch_A);
  #if defined(IN_LAYOUT_HWC)
  {
    const uint WG_TILE_Y = SG_M * WG_M;

    const uint IN_CH8 = (IN_CH + 7u) / 8u;
    const uint RSC8 = KERNEL_X * KERNEL_Y * IN_CH8;

    // NOTE: SG_K-aware base (must match prefetch_B / microkernel traversal)
    const uint k8_base = subgroupBroadcastFirst(kk * (SG_K * INVOC_K8));

    for (uint sgm = 0; sgm < SG_M; ++sgm) {
      const uint yLocal = sgmi * SG_M + sgm;
      const uint out_y = wgxy.y + yLocal;

      for (uint sgk = 0; sgk < SG_K; ++sgk) {
        for (uint k8Local = sgni; k8Local < INVOC_K8; k8Local += WG_N) {
          const uint k8TileLocal = sgk * INVOC_K8 + k8Local;
          const uint k8 = k8_base + k8TileLocal;

          if (k8 >= RSC8) {
            for (uint xLocal = gl_SubgroupInvocationID; xLocal < INVOC_M; xLocal += SG_SIZE) {
              const uint shIndex = ((k8TileLocal * WG_TILE_Y + yLocal) * INVOC_M) + xLocal;
              sh_a[pp][shIndex] = uvec4(0);
            }
            continue;
          }

          const uint zrs = k8 / IN_CH8;
          const uint zc8 = k8 - zrs * IN_CH8;

          const uint zr = zrs / KERNEL_X;
          const uint zs = zrs - zr * KERNEL_X;

          for (uint xLocal = gl_SubgroupInvocationID; xLocal < INVOC_M; xLocal += SG_SIZE) {
            const uint out_x = wgxy.x + xLocal;
            uvec4 v = uvec4(0);

            if (out_x < pc.IN_W && out_y < pc.IN_H) {
              const uint in_y = out_y + zr - PADDING_Y;
              const uint in_x = out_x + zs - PADDING_X;

              if (in_y < pc.IN_H && in_x < pc.IN_W) {
                const uint c0 = zc8 * 8u;
                const uint base = in_y * (pc.IN_W * IN_CH) + in_x * IN_CH;

                if (c0 + 7u < IN_CH) {
                  uint16_t a0 = input_tensor_blob[base + (c0 + 0u)];
                  uint16_t a1 = input_tensor_blob[base + (c0 + 1u)];
                  uint16_t a2 = input_tensor_blob[base + (c0 + 2u)];
                  uint16_t a3 = input_tensor_blob[base + (c0 + 3u)];
                  uint16_t a4 = input_tensor_blob[base + (c0 + 4u)];
                  uint16_t a5 = input_tensor_blob[base + (c0 + 5u)];
                  uint16_t a6 = input_tensor_blob[base + (c0 + 6u)];
                  uint16_t a7 = input_tensor_blob[base + (c0 + 7u)];

                  v.x = packUint2x16(u16vec2(a0, a1));
                  v.y = packUint2x16(u16vec2(a2, a3));
                  v.z = packUint2x16(u16vec2(a4, a5));
                  v.w = packUint2x16(u16vec2(a6, a7));
                } else {
                  uint16_t a0 = (c0 + 0u) < IN_CH ? input_tensor_blob[base + (c0 + 0u)] : uint16_t(0);
                  uint16_t a1 = (c0 + 1u) < IN_CH ? input_tensor_blob[base + (c0 + 1u)] : uint16_t(0);
                  uint16_t a2 = (c0 + 2u) < IN_CH ? input_tensor_blob[base + (c0 + 2u)] : uint16_t(0);
                  uint16_t a3 = (c0 + 3u) < IN_CH ? input_tensor_blob[base + (c0 + 3u)] : uint16_t(0);
                  uint16_t a4 = (c0 + 4u) < IN_CH ? input_tensor_blob[base + (c0 + 4u)] : uint16_t(0);
                  uint16_t a5 = (c0 + 5u) < IN_CH ? input_tensor_blob[base + (c0 + 5u)] : uint16_t(0);
                  uint16_t a6 = (c0 + 6u) < IN_CH ? input_tensor_blob[base + (c0 + 6u)] : uint16_t(0);
                  uint16_t a7 = (c0 + 7u) < IN_CH ? input_tensor_blob[base + (c0 + 7u)] : uint16_t(0);

                  v.x = packUint2x16(u16vec2(a0, a1));
                  v.y = packUint2x16(u16vec2(a2, a3));
                  v.z = packUint2x16(u16vec2(a4, a5));
                  v.w = packUint2x16(u16vec2(a6, a7));
                }
              }
            }

            const uint shIndex = ((k8TileLocal * WG_TILE_Y + yLocal) * INVOC_M) + xLocal;
            sh_a[pp][shIndex] = v;
          }
        }
      }
    }
  }
  #elif defined(IN_LAYOUT_HWC8)
  { // HWC8 case (extended to SG_K)
    const uint WG_TILE_Y = SG_M * WG_M;

    const uint IN_CH8 = IN_CH / 8u;
    const uint RSC8 = (KERNEL_X * KERNEL_Y) * IN_CH8;

    const uint K8_TILE = SG_K * INVOC_K8;

    // Split the k8-slices across WG_N subgroups (no duplication across N).
    // Assumption: K8_TILE % WG_N == 0
    const uint K8_PER_SUBGROUP = K8_TILE / WG_N;

    // Base k8vec for this kk iteration (uniform in the subgroup)
    const uint k8_base = subgroupBroadcastFirst(kk * K8_TILE);

    // Total vector loads performed by THIS subgroup
    const uint TOTAL = SG_M * INVOC_M * K8_PER_SUBGROUP;

    // If true, the last kk tile is never partial (so k8 < RSC8 always holds here).
    const bool K8_TILE_ALIGNED = (RSC8 % K8_TILE) == 0u;

    for (uint idx = gl_SubgroupInvocationID; idx < TOTAL; idx += SG_SIZE) {
      // De-linearize idx -> (sgm, xLocal, t)
      const uint t = idx % K8_PER_SUBGROUP; // 0..K8_PER_SUBGROUP-1
      const uint tmp = idx / K8_PER_SUBGROUP;
      const uint xLocal = tmp % INVOC_M; // 0..INVOC_M-1
      const uint sgm = tmp / INVOC_M; // 0..SG_M-1

      const uint yLocal = sgmi * SG_M + sgm;
      const uint out_y = wgxy.y + yLocal;
      const uint out_x = wgxy.x + xLocal;

      // k8TileLocal in [0, K8_TILE)
      const uint k8TileLocal = sgni * K8_PER_SUBGROUP + t;
      const uint k8 = k8_base + k8TileLocal;

      vstype v = vstype(0);

      // Tail handling for partial last kk tile (if any).
      if (K8_TILE_ALIGNED || (k8 < RSC8)) {
        // Decode k8 -> (r,s,c8) in RSC8 domain
        const uint zrs = k8 / IN_CH8;
        const uint zc8 = k8 - zrs * IN_CH8;

        const uint zr = zrs / KERNEL_X;
        const uint zs = zrs - zr * KERNEL_X;

        // Spatial bounds / padding handling.
        const uint in_y = out_y + zr - PADDING_Y;
        const uint in_x = out_x + zs - PADDING_X;

        if (in_y < pc.IN_H && in_x < pc.IN_W) {
          // HWC8 global index: [y][x][c8]
          const uint o = in_y * (pc.IN_W * IN_CH8) + in_x * IN_CH8 + zc8;
          v = input_tensor_blob[o];
        }
      }

      // Canonical shared layout: sh_a[k8TileLocal][yLocal][xLocal]
      const uint shIndex = ((k8TileLocal * WG_TILE_Y + yLocal) * INVOC_M) + xLocal;
      sh_a[pp][shIndex] = v;
    }
  }

  #elif defined(IN_LAYOUT_CHWC8)
  { // CHWC8 case (extended to SG_K)
    const uint WG_TILE_Y = SG_M * WG_M;

    // CHWC8 guarantees IN_CH divisible by 8.
    const uint IN_CH8 = IN_CH / 8u;
    const uint IN_HW = pc.IN_W * pc.IN_H;

    const uint RSC8 = (KERNEL_X * KERNEL_Y) * IN_CH8;
    const uint K8_TILE = SG_K * INVOC_K8;

    // Stripe k8 slices across WG_N subgroups (sgni).
    // Assumption: K8_TILE % WG_N == 0
    const uint K8_PER_SUBGROUP = K8_TILE / WG_N;

    // Base k8vec for this kk iteration (uniform in the subgroup).
    const uint k8_base = subgroupBroadcastFirst(kk * K8_TILE);

    // Total number of vector loads performed by THIS subgroup for A:
    // SG_M rows (within this sgmi slab) * INVOC_M x-positions * K8_PER_SUBGROUP k8-slices
    const uint TOTAL = SG_M * INVOC_M * K8_PER_SUBGROUP;

    // If true, the last kk tile is never partial (so k8 < RSC8 always holds here).
    const bool K8_TILE_ALIGNED = (RSC8 % K8_TILE) == 0u;

    for (uint idx = gl_SubgroupInvocationID; idx < TOTAL; idx += SG_SIZE) {
      // Make X the fastest varying index to maximize coalescing for CHWC8.
      const uint xLocal = idx % INVOC_M; // 0..INVOC_M-1
      const uint tmp = idx / INVOC_M;
      const uint sgm = tmp % SG_M; // 0..SG_M-1
      const uint t = tmp / SG_M; // 0..K8_PER_SUBGROUP-1

      const uint yLocal = sgmi * SG_M + sgm;
      const uint out_y = wgxy.y + yLocal;
      const uint out_x = wgxy.x + xLocal;

      // k8TileLocal in [0, K8_TILE)
      const uint k8TileLocal = sgni * K8_PER_SUBGROUP + t;
      const uint k8 = k8_base + k8TileLocal;

      vstype v = vstype(0);

      if (K8_TILE_ALIGNED || (k8 < RSC8)) {
        // Decode k8 -> (r,s,c8) in RSC8 domain.
        const uint zrs = k8 / IN_CH8;
        const uint zc8 = k8 - zrs * IN_CH8;

        const uint zr = zrs / KERNEL_X;
        const uint zs = zrs - zr * KERNEL_X;

        // Apply spatial padding. (uint underflow is intended.)
        const uint in_y = out_y + zr - PADDING_Y;
        const uint in_x = out_x + zs - PADDING_X;

        if (in_y < pc.IN_H && in_x < pc.IN_W) {
          // CHWC8 global index: [c8][y][x]
          const uint o = zc8 * IN_HW + in_y * pc.IN_W + in_x;
          v = input_tensor_blob[o];
        }
      }

      // Canonical shared layout: sh_a[k8TileLocal][yLocal][xLocal]
      const uint shIndex = ((k8TileLocal * WG_TILE_Y + yLocal) * INVOC_M) + xLocal;
      sh_a[pp][shIndex] = v;
    }
  }
  #endif
  #pragma end_block; // prefetch_A

  #pragma begin_block(prefetch_B);
  {
    const uint W = (VSTYPE_SIZE / ATYPE_SIZE); // must be 8 for uvec4+f16
    // Sanity requirement for this kernel design:
    // INVOC_K must be a multiple of 8 so INVOC_K8 is exact.
    // (Your config uses INVOC_K=16, so OK.)

    const uint K8_TILE = SG_K * INVOC_K8; // number of RSC8 vectors per kk iteration
    const uint n8Base = wgni / W; // output-channel block base for this workgroup

    const bool K8_TILE_ALIGNED = (RSC8 % K8_TILE) == 0u;

    for (uint i = gl_LocalInvocationIndex; i < SH_B_SIZE; i += WG_SIZE) {
      // sh_b is laid out as [kScalarLocal][n8Local]
      const uint kScalarLocal = i / WG_TILE_N8; // 0 .. SG_K*INVOC_K-1
      const uint n8Local = i - kScalarLocal * WG_TILE_N8;

      // Decode scalar-K row into (k8TileLocal, c_lane)
      const uint k8TileLocal = kScalarLocal >> 3; // /8
      const uint c_lane = kScalarLocal & 7u; // %8

      // Global RSC8 vector index for this kk tile
      const uint k8Global = kk * K8_TILE + k8TileLocal;

      // Global output-channel block
      const uint n8Global = n8Base + n8Local;

      const bool n_ok = (n8Global < OUT_CH8);

      // This matches your host layout:
      // uvec4 index = ((((r*S+s)*C8 + c8)*K8 + k8) * 8 + c_lane)
      // where k8Global == ((r*S+s)*C8 + c8), and K8 == OUT_CH8.
      const uint filtIdx = ((k8Global * OUT_CH8 + n8Global) << 3) + c_lane;

      if (K8_TILE_ALIGNED) {
        sh_b[pp][i] = n_ok ? filter_blob[filtIdx] : vstype(0);
      } else {
        sh_b[pp][i] = (n_ok && (k8Global < RSC8)) ? filter_blob[filtIdx] : vstype(0);
      }
    }
  }
  #pragma end_block; // prefetch_B

  #pragma begin_block(gemm);
  {
    const uint W = (VSTYPE_SIZE / ATYPE_SIZE); // 8
    const uint K8_TILE = SG_K * INVOC_K8; // number of k8 vectors in this kk tile
    const uint WG_TILE_Y_LOCAL = (SG_M * WG_M);

    for (uint zn = 0; zn < SG_N; ++zn) {
      const uint n8LocalBase = (sgni * SG_N + zn) * INVOC_N8; // within WG_TILE_N8

      for (uint zm = 0; zm < SG_M; ++zm) {
        const uint yLocal = sgmi * SG_M + zm;

        // must match your writeback mapping
        for (uint idx = gl_SubgroupInvocationID; idx < (INVOC_M * INVOC_N8); idx += SG_SIZE) {
          const uint i = idx / SG_SIZE;
          const uint xLocal = idx / INVOC_N8;
          const uint n8 = idx - xLocal * INVOC_N8;
          const uint n8Local = n8LocalBase + n8;

          // Accumulator in f16 registers (8 output channels as 4x f16vec2)
          const uvec4 sBits = sum[zn][zm][i];
          f16vec2 acc01 = f16vec2(unpackHalf2x16(sBits.x));
          f16vec2 acc23 = f16vec2(unpackHalf2x16(sBits.y));
          f16vec2 acc45 = f16vec2(unpackHalf2x16(sBits.z));
          f16vec2 acc67 = f16vec2(unpackHalf2x16(sBits.w));

          for (uint k8TileLocal = 0; k8TileLocal < K8_TILE; ++k8TileLocal) {
            // A: one uvec4 == 8 fp16 input channels
            const uint aIndex = ((k8TileLocal * WG_TILE_Y_LOCAL + yLocal) * INVOC_M) + xLocal;
            const uvec4 aBits = sh_a[ppx][aIndex];

            const f16vec2 a01 = f16vec2(unpackHalf2x16(aBits.x));
            const f16vec2 a23 = f16vec2(unpackHalf2x16(aBits.y));
            const f16vec2 a45 = f16vec2(unpackHalf2x16(aBits.z));
            const f16vec2 a67 = f16vec2(unpackHalf2x16(aBits.w));

            const float16_t a0 = a01.x;
            const float16_t a1 = a01.y;
            const float16_t a2 = a23.x;
            const float16_t a3 = a23.y;
            const float16_t a4 = a45.x;
            const float16_t a5 = a45.y;
            const float16_t a6 = a67.x;
            const float16_t a7 = a67.y;

            // B: scalar-K rows for this k8 vector: kScalarLocal = k8TileLocal*8 + c_lane
            const uint kScalarBaseLocal = k8TileLocal * W;

            // c_lane = 0
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 0u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a0);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a0);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a0);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a0);
            }
            // c_lane = 1
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 1u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a1);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a1);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a1);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a1);
            }
            // c_lane = 2
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 2u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a2);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a2);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a2);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a2);
            }
            // c_lane = 3
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 3u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a3);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a3);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a3);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a3);
            }
            // c_lane = 4
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 4u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a4);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a4);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a4);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a4);
            }
            // c_lane = 5
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 5u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a5);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a5);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a5);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a5);
            }
            // c_lane = 6
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 6u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a6);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a6);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a6);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a6);
            }
            // c_lane = 7
            {
              const uvec4 bBits = sh_b[ppx][(kScalarBaseLocal + 7u) * WG_TILE_N8 + n8Local];
              acc01 += f16vec2(unpackHalf2x16(bBits.x)) * f16vec2(a7);
              acc23 += f16vec2(unpackHalf2x16(bBits.y)) * f16vec2(a7);
              acc45 += f16vec2(unpackHalf2x16(bBits.z)) * f16vec2(a7);
              acc67 += f16vec2(unpackHalf2x16(bBits.w)) * f16vec2(a7);
            }
          }

          // Pack f16 accumulator back to your uvec4 accumulator storage
          sum[zn][zm][i] = uvec4(
              packHalf2x16(vec2(acc01)),
              packHalf2x16(vec2(acc23)),
              packHalf2x16(vec2(acc45)),
              packHalf2x16(vec2(acc67))
            );
        }
      }
    }
  }
  #pragma end_block;

  #pragma begin_block(activation);
  #if defined(ACTIVATION_NONE)
  #elif defined(ACTIVATION_ReLU)
  for (uint zn = 0; zn < SG_N; ++zn) {
    for (uint zm = 0; zm < SG_M; ++zm) {
      for (uint i = 0; i < VECS_PER_INVOC; ++i) {
        uvec4 v = sum[zn][zm][i];
        f16vec2 a0 = unpackFloat2x16(v.x);
        f16vec2 a1 = unpackFloat2x16(v.y);
        f16vec2 a2 = unpackFloat2x16(v.z);
        f16vec2 a3 = unpackFloat2x16(v.w);

        a0.x = max(a0.x, atype(0.0f));
        a0.y = max(a0.y, atype(0.0f));

        a1.x = max(a1.x, atype(0.0f));
        a1.y = max(a1.y, atype(0.0f));

        a2.x = max(a2.x, atype(0.0f));
        a2.y = max(a2.y, atype(0.0f));

        a3.x = max(a3.x, atype(0.0f));
        a3.y = max(a3.y, atype(0.0f));

        v.x = packFloat2x16(a0);
        v.y = packFloat2x16(a1);
        v.z = packFloat2x16(a2);
        v.w = packFloat2x16(a3);
        sum[zn][zm][i] = v;
      }
    }
  }
  #else
  UNDEFINED_ACTIVATION;
  #endif
  // TODO: Apply activation function to all output channels.
  #pragma end_block;

  #pragma begin_block(writeback);
  #if defined(OUT_LAYOUT_HWC)
  // ostype: uint16_t
  {
    const uint W = (VSTYPE_SIZE / ATYPE_SIZE); // 8 for uvec4+f16
    const uint INVOC_N8 = INVOC_N / W;

    for (uint zm = 0; zm < SG_M; ++zm) {
      const uint y = wgxy.y + sgmi * SG_M + zm;
      if (y >= pc.IN_H) break;

      const uint gy = y * pc.IN_W * OUT_CH;

      for (uint zn = 0; zn < SG_N; ++zn) {
        const uint base_c = wgni + (sgni * SG_N + zn) * INVOC_N;

        // Iterate fragments owned by this lane: idx = lane + i*SG_SIZE
        for (uint idx = gl_SubgroupInvocationID; idx < (INVOC_M * INVOC_N8); idx += SG_SIZE) {
          const uint i = idx / SG_SIZE;
          const uint x = idx / INVOC_N8; // 0..INVOC_M-1
          const uint n8 = idx % INVOC_N8; // 0..INVOC_N8-1

          const uint zx = wgxy.x + x;
          if (zx >= pc.IN_W) continue;

          const uint c0 = base_c + n8 * W; // scalar channel index of lane's 8-wide block
          if (c0 >= OUT_CH) continue;

          // Load the 8 half values (as bits) from your accumulator
          uvec4 v = sum[zn][zm][i];

          u16vec2 vx = unpackUint2x16(v.x); // channels c0+0, c0+1
          u16vec2 vy = unpackUint2x16(v.y); // channels c0+2, c0+3
          u16vec2 vz = unpackUint2x16(v.z); // channels c0+4, c0+5
          u16vec2 vw = unpackUint2x16(v.w); // channels c0+6, c0+7

          const uint go = gy + zx * OUT_CH + c0;

          // Fast path: full 8 channels in bounds
          if (c0 + 7u < OUT_CH) {
            output_tensor_blob[go + 0u] = vx.x;
            output_tensor_blob[go + 1u] = vx.y;
            output_tensor_blob[go + 2u] = vy.x;
            output_tensor_blob[go + 3u] = vy.y;
            output_tensor_blob[go + 4u] = vz.x;
            output_tensor_blob[go + 5u] = vz.y;
            output_tensor_blob[go + 6u] = vw.x;
            output_tensor_blob[go + 7u] = vw.y;
          } else {
            // Tail channels
            if (c0 + 0u < OUT_CH) output_tensor_blob[go + 0u] = vx.x;
            if (c0 + 1u < OUT_CH) output_tensor_blob[go + 1u] = vx.y;
            if (c0 + 2u < OUT_CH) output_tensor_blob[go + 2u] = vy.x;
            if (c0 + 3u < OUT_CH) output_tensor_blob[go + 3u] = vy.y;
            if (c0 + 4u < OUT_CH) output_tensor_blob[go + 4u] = vz.x;
            if (c0 + 5u < OUT_CH) output_tensor_blob[go + 5u] = vz.y;
            if (c0 + 6u < OUT_CH) output_tensor_blob[go + 6u] = vw.x;
            if (c0 + 7u < OUT_CH) output_tensor_blob[go + 7u] = vw.y;
          }
        }
      }
    }
  }

  #elif defined(OUT_LAYOUT_HWC8)
  // ostype: uvec4 <=> vstype: uvec4
  // atype : float16_t
  // => HWC8 layout. => OUT_CH % 8 == 0
  {
    const uint W = (VSTYPE_SIZE / ATYPE_SIZE); // 8 for uvec4+f16
    const uint INVOC_N8 = INVOC_N / W;
    const uint OUT_CH8 = OUT_CH / W; // guaranteed integer for HWC8

    for (uint zm = 0; zm < SG_M; ++zm) {
      const uint y = wgxy.y + sgmi * SG_M + zm;
      if (y >= pc.IN_H) break;

      const uint gy8 = y * pc.IN_W * OUT_CH8;

      for (uint zn = 0; zn < SG_N; ++zn) {
        // scalar channel base for this zn tile
        const uint base_c = wgni + (sgni * SG_N + zn) * INVOC_N;
        const uint base_c8 = base_c / W; // vector channel base (c/8), safe because HWC8 implies /8 alignment

        for (uint idx = gl_SubgroupInvocationID; idx < (INVOC_M * INVOC_N8); idx += SG_SIZE) {
          const uint i = idx / SG_SIZE;
          const uint x = idx / INVOC_N8; // 0..INVOC_M-1
          const uint n8 = idx % INVOC_N8; // 0..INVOC_N8-1

          const uint zx = wgxy.x + x;
          if (zx >= pc.IN_W) continue;

          const uint c8 = base_c8 + n8;
          if (c8 >= OUT_CH8) continue; // should only matter on the last channel tile

          // Direct vector store: one uvec4 == 8 fp16 channels
          output_tensor_blob[gy8 + zx * OUT_CH8 + c8] = sum[zn][zm][i];
        }
      }
    }
  }

  #elif defined(OUT_LAYOUT_CHWC8)
  // ostype: uvec4 <=> vstype
  {
    const uint W = (VSTYPE_SIZE / ATYPE_SIZE); // 8 for uvec4 + f16
    const uint INVOC_N8 = INVOC_N / W;
    const uint OUT_CH8 = OUT_CH / W;
    const uint IN_HW = pc.IN_W * pc.IN_H;

    for (uint zm = 0; zm < SG_M; ++zm) {
      const uint y = wgxy.y + sgmi * SG_M + zm;
      if (y >= pc.IN_H) break;

      for (uint zn = 0; zn < SG_N; ++zn) {
        // vector-channel base (c8) for this (wg, sg, zn) tile
        const uint base_c = wgni + (sgni * SG_N + zn) * INVOC_N; // scalar
        const uint base_c8 = base_c / W; // vector index

        for (uint idx = gl_SubgroupInvocationID; idx < (INVOC_M * INVOC_N8); idx += SG_SIZE) {
          const uint i = idx / SG_SIZE;
          const uint x = idx / INVOC_N8; // 0..INVOC_M-1
          const uint n8 = idx % INVOC_N8; // 0..INVOC_N8-1

          const uint gx = wgxy.x + x;
          if (gx >= pc.IN_W) continue;

          const uint c8 = base_c8 + n8;
          // Per your assumption: OUT_CH divisible by 8. This check is only needed for the last tile in N.
          if (c8 >= OUT_CH8) continue;

          output_tensor_blob[c8 * IN_HW + y * pc.IN_W + gx] = sum[zn][zm][i];
        }
      }
    }
  }
  #else
  UNDEFINED_OUT_LAYOUT;
  #endif
  #pragma end_block;

  #pragma inline_block(load_bias);

  #if defined(NASYNC_READ)
  for (; kk < KK; ++kk) {
    #pragma inline_block(prefetch_A); // depends on correct kk
    #pragma inline_block(prefetch_B); // depends on correct kk
    barrier();
    #pragma inline_block(gemm); // <- doesn't depend on kk, reads ppx
    barrier();
  }
  #elif defined(ASYNC_READ)
  // prolog
  pp = 0;
  kk = 0;
  #pragma inline_block(prefetch_A);
  #pragma inline_block(prefetch_B);
  ++kk;
  barrier();
  for (; kk < KK; ++kk) {
    ppx = pp;
    pp = pp ^ 1; // <- load into ping pong 0.
    // stead state
    #pragma inline_block(prefetch_A); // depends on correct kk
    #pragma inline_block(prefetch_B); // depends on correct kk
    #pragma inline_block(gemm); // <- doesn't depend on kk, reads ppx
    barrier();
  }
  // epilog
  ppx = pp;
  #pragma inline_block(gemm); // <- doesn't depend on kk, reads ppx
  #else
  UNDEFINED_PIPELINE_STRAT;
  #endif

  // writeback + activation
  #pragma inline_block(activation);
  #pragma inline_block(writeback);
}
