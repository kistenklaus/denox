#version 460

// TODO: Investigate properly pipelined execution.
// The idea would be to double the shared memory footprint of SH_A and SH_B
// and make a ping pong buffer within them. The hole idea here is do remove
// one barrier from the main kk loop, this might actually improve performance,
// but it's really hard to tell, at this point if we read asyn or not doesn't make any
// difference and the extra barrier might be the reason for this.
//
// Another thing to consider here is that it might just be possible to
// only apply a ping pong stategy to one side (e.g. sh_a), but this
// might require a more complex rework not completely sure.

#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_memory_scope_semantics : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_explicit_arithmetic_types_int16 : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_16bit_storage                   : require
#extension GL_EXT_scalar_block_layout : require
#extension GL_KHR_shader_subgroup_basic                  : enable
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_shuffle_relative : require
#extension GL_EXT_shader_subgroup_extended_types_int16 : require
#extension GL_KHR_cooperative_matrix : require

#pragma use_vulkan_memory_model

// Vectorized memory storage type.
// NOTE: After loading from global memory we can always vectorize.
// For example for register to shared memory copy or coopMatLoad from shared memory.
#define vstype uvec4
#define VSTYPE_SIZE 16

layout(set = A_SET, binding = A_BINDING, std430) readonly buffer a_input_buf {
  a_istype a_input_tensor_blob[];
};

layout(set = B_SET, binding = B_BINDING, std430) readonly buffer b_input_buf {
  b_istype b_input_tensor_blob[];
};

layout(set = OUTPUT_SET, binding = OUTPUT_BINDING, std430) writeonly buffer output_buf {
  ostype output_tensor_blob[];
};

layout(set = A_FILTER_SET, binding = A_FILTER_BINDING, std430) readonly buffer a_filter_buf {
  a_fstype a_filter_blob[];
};

layout(set = B_FILTER_SET, binding = B_FILTER_BINDING, std430) readonly buffer b_filter_buf {
  b_fstype b_filter_blob[];
};

#ifdef USE_BIAS
layout(set = BIAS_SET, binding = BIAS_BINDING, std430) readonly buffer bias_buf {
  // NOTE: The bias blob is required to be aligned to 16byte and
  // padded to a multiple of CM_N.
  // This allows us to skip shared memory when loading the bias.
  vstype bias_blob[];
};
#endif

layout(push_constant) uniform PushConstant {
  uint IN_W;
  uint IN_H;
} pc;

// shared uvec4 sh_input[WG_M][SG_M * SG_K * CM_M * CM_K / 4];
// shared uvec4 sh_filter[WG_N][SG_K * SG_N * CM_K * CM_N / 4];

// NOTE: Static shared memory allocation.
// We require the shared memory buffers
// - sh_out: to writeback the output while performing out of bound checks.
// - sh_input : for the input subtile.
// - sh_filter : for the filter subtile.
// Instead of defining all 3, we define a single buffer, which we reuse
// across different stages, because sh_out and (sh_input, sh_filter) are
// used at mutally exclusive situations.

const uint WG_SIZE = SG_SIZE * SG_COUNT;

const uint SG_TILE_X = CM_M;
const uint SG_TILE_Y = SG_M;

const uint WG_TILE_X = SG_TILE_X;
const uint WG_TILE_Y = SG_TILE_Y * WG_M;
const uint WG_TILE_N = CM_N * SG_N * WG_N;

const uint A_RSC = KERNEL_X * KERNEL_Y * A_IN_CH;
const uint B_RSC = KERNEL_X * KERNEL_Y * B_IN_CH;

const uint PREFETCH_A_QQ = ((CM_M * A_CM_K / (VSTYPE_SIZE / ATYPE_SIZE)) * A_SG_K * SG_M);
const uint PREFETCH_A_SQQ = PREFETCH_A_QQ / WG_N;
const uint PREFETCH_A_IQQ = (PREFETCH_A_SQQ + SG_SIZE - 1) / SG_SIZE;

const uint PREFETCH_B_QQ = ((CM_M * B_CM_K / (VSTYPE_SIZE / ATYPE_SIZE)) * B_SG_K * SG_M);
const uint PREFETCH_B_SQQ = PREFETCH_B_QQ / WG_N;
const uint PREFETCH_B_IQQ = (PREFETCH_B_SQQ + SG_SIZE - 1) / SG_SIZE;

const uint PREFETCH_A_FILTER_QQ = (A_CM_K * CM_N / (VSTYPE_SIZE / ATYPE_SIZE)) * A_SG_K * SG_N;
const uint PREFETCH_A_FILTER_SQQ = PREFETCH_A_FILTER_QQ / WG_M;
const uint PREFETCH_A_FILTER_IQQ = (PREFETCH_A_FILTER_SQQ + SG_SIZE - 1) / SG_SIZE;

const uint PREFETCH_B_FILTER_QQ = (B_CM_K * CM_N / (VSTYPE_SIZE / ATYPE_SIZE)) * B_SG_K * SG_N;
const uint PREFETCH_B_FILTER_SQQ = PREFETCH_B_FILTER_QQ / WG_M;
const uint PREFETCH_B_FILTER_IQQ = (PREFETCH_B_FILTER_SQQ + SG_SIZE - 1) / SG_SIZE;

const uint SH_OUT_SG_SIZE8 = SG_M * SG_N * CM_M * CM_N / (VSTYPE_SIZE / ATYPE_SIZE);
const uint SH_OUT_SIZE8 = SG_COUNT * SH_OUT_SG_SIZE8;

const uint SH_A_SIZE8 = WG_M * PREFETCH_A_QQ;
const uint SH_A_FILTER_SIZE8 = WG_N * PREFETCH_A_FILTER_QQ;

const uint SH_B_SIZE8 = WG_M * PREFETCH_B_QQ;
const uint SH_B_FILTER_SIZE8 = WG_N * PREFETCH_B_FILTER_QQ;

const uint SH_A_TOTAL_SIZE8 = SH_A_SIZE8 + SH_A_FILTER_SIZE8;
const uint SH_B_TOTAL_SIZE8 = SH_B_SIZE8 + SH_B_FILTER_SIZE8;
const uint SH_AB_MAX_SIZE8 = (SH_A_TOTAL_SIZE8 > SH_B_TOTAL_SIZE8) ? SH_A_TOTAL_SIZE8 : SH_B_TOTAL_SIZE8;
const uint SH_BUF_SIZE8 = (SH_AB_MAX_SIZE8 > SH_OUT_SIZE8) ? SH_AB_MAX_SIZE8 : SH_OUT_SIZE8;

shared vstype sh_buf[SH_BUF_SIZE8];

const uint SH_OUT_OFFSET = 0;

const uint SH_A_OFFSET = 0;
const uint SH_A_FILTER_OFFSET = SH_A_OFFSET + SH_A_SIZE8;

const uint SH_B_OFFSET = 0;
const uint SH_B_FILTER_OFFSET = SH_B_OFFSET + SH_B_SIZE8;

const uint CM_M8 = CM_M / 8;
const uint A_CM_K8 = A_CM_K / 8;
const uint B_CM_K8 = B_CM_K / 8;
const uint CM_N8 = CM_N / 8;

// x : channel output tile.
// y : x output / input tile.
// z : y output / input tile.
layout(local_size_x = SG_SIZE, local_size_y = SG_COUNT, local_size_z = 1) in;
void main() {
  const uvec3 wgi = gl_WorkGroupID.xyz;
  const uint sgi = gl_SubgroupID;
  const uvec2 wgxy = uvec2(wgi.y * WG_TILE_X, wgi.z * WG_TILE_Y);
  const uint wgni = WG_TILE_N * wgi.x;
  const uint sgmi = (sgi / WG_N);
  const uint sgni = sgi % WG_N;

  #pragma begin_block(load_bias);
  #ifdef USE_BIAS
  for (uint zn = 0; zn < SG_N; zn++) {
    coopmat < atype, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator > bias =
      coopmat < atype, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator > (0.0f);
    const uint offset = (wgni + (sgni * SG_N + zn) * CM_N) / (VSTYPE_SIZE / ATYPE_SIZE);
    const uint OUT_CH_PADDED = (OUT_CH + CM_N - 1) / CM_N * CM_N;
    if (offset < (OUT_CH_PADDED / (VSTYPE_SIZE / ATYPE_SIZE))) {
      coopMatLoad(bias,
        bias_blob,
        offset,
        0,
        gl_CooperativeMatrixLayoutRowMajor);
    }
    for (uint zm = 0; zm < SG_M; zm++) {
      sum[zn][zm] = bias;
    }
  }
  #else
  for (uint zn = 0; zn < SG_N; zn++) {
    for (uint zm = 0; zm < SG_M; zm++) {
      sum[zn][zm] = coopmat < atype, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator > (0.0f);
    }
  }
  #endif
  #pragma end_block; // load_bias

  #pragma begin_block(prefetch_A);
  #if VSTYPE_SIZE == 16
  #if defined(A_IN_LAYOUT_HWC)
  {
    #define FEATURE_LAYOUT_COLUMN_MAJOR
    #define FEATURE_LAYOUT_LINEAR
    #define FEATURE_LAYOUT_RSC
    const uint k = subgroupBroadcastFirst(A_CM_K * A_SG_K * kk);
    for (uint q = 0; q < PREFETCH_A_IQQ; ++q) {
      uvec4 v = uvec4(0);
      const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
      if (liq8 < PREFETCH_A_SQQ) {
        const uint siq8 = sgni * PREFETCH_A_SQQ + liq8;
        const uint sgm = siq8 / (CM_M8 * A_CM_K * A_SG_K);
        const uint sgij8 = siq8 % (CM_M8 * A_CM_K * A_SG_K);
        const uint sgk = (sgij8 / CM_M8);
        const uint zm8 = (sgij8 % CM_M8);
        const uint zk = k + sgk;
        const uint zrs = zk / A_IN_CH;
        const uint zc = zk % A_IN_CH;
        const uint zr = zrs / KERNEL_X;
        const uint zs = zrs % KERNEL_X;
        const uint zy = wgxy.y + sgmi * SG_M + sgm + zr - PADDING_Y;
        const uint zx = wgxy.x + zs + zm8 * 8 - PADDING_X;
        if (zy < pc.IN_H && zk < KERNEL_X * KERNEL_Y * A_IN_CH) {
          const uint o = zy * (pc.IN_W * A_IN_CH) + zx * A_IN_CH + zc;
          if ((zx < pc.IN_W && zx + 8 < pc.IN_W)) {
            uint16_t a = a_input_tensor_blob[o + 0 * A_IN_CH];
            uint16_t b = a_input_tensor_blob[o + 1 * A_IN_CH];
            uint ab = packUint2x16(u16vec2(a, b));
            uint16_t c = a_input_tensor_blob[o + 2 * A_IN_CH];
            uint16_t d = a_input_tensor_blob[o + 3 * A_IN_CH];
            uint cd = packUint2x16(u16vec2(c, d));
            uint16_t e = a_input_tensor_blob[o + 4 * A_IN_CH];
            uint16_t f = a_input_tensor_blob[o + 5 * A_IN_CH];
            uint ef = packUint2x16(u16vec2(e, f));
            uint16_t g = a_input_tensor_blob[o + 6 * A_IN_CH];
            uint16_t h = a_input_tensor_blob[o + 7 * A_IN_CH];
            uint gh = packUint2x16(u16vec2(g, h));
            v = uvec4(ab, cd, ef, gh);
          } else {
            uint16_t a = (zx + 0) < pc.IN_W ? a_input_tensor_blob[o + 0 * A_IN_CH] : uint16_t(0);
            uint16_t b = (zx + 1) < pc.IN_W ? a_input_tensor_blob[o + 1 * A_IN_CH] : uint16_t(0);
            uint ab = packUint2x16(u16vec2(a, b));
            uint16_t c = (zx + 2) < pc.IN_W ? a_input_tensor_blob[o + 2 * A_IN_CH] : uint16_t(0);
            uint16_t d = (zx + 3) < pc.IN_W ? a_input_tensor_blob[o + 3 * A_IN_CH] : uint16_t(0);
            uint cd = packUint2x16(u16vec2(c, d));
            uint16_t e = (zx + 4) < pc.IN_W ? a_input_tensor_blob[o + 4 * A_IN_CH] : uint16_t(0);
            uint16_t f = (zx + 5) < pc.IN_W ? a_input_tensor_blob[o + 5 * A_IN_CH] : uint16_t(0);
            uint ef = packUint2x16(u16vec2(e, f));
            uint16_t g = (zx + 6) < pc.IN_W ? a_input_tensor_blob[o + 6 * A_IN_CH] : uint16_t(0);
            uint16_t h = (zx + 7) < pc.IN_W ? a_input_tensor_blob[o + 7 * A_IN_CH] : uint16_t(0);
            uint gh = packUint2x16(u16vec2(g, h));
            v = uvec4(ab, cd, ef, gh);
          }
        }
      }
      prefetch_feature[q] = v;
    }
  }
  #elif defined(A_IN_LAYOUT_HWC8)
  {
    #define FEATURE_LAYOUT_ROW_MAJOR
    #define FEATURE_LAYOUT_LINEAR
    #define FEATURE_LAYOUT_RSC
    {
      const uint k8 = subgroupBroadcastFirst(A_CM_K8 * A_SG_K * kk);
      const uint A_IN_CH8 = A_IN_CH / 8;
      for (uint q = 0; q < PREFETCH_A_IQQ; ++q) {
        uvec4 v = uvec4(0);
        const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
        if (liq8 < PREFETCH_A_SQQ) {
          const uint siq8 = sgni * PREFETCH_A_SQQ + liq8;
          const uint sgm = siq8 / (CM_M * A_CM_K8 * A_SG_K);
          const uint sgij8 = siq8 % (CM_M * A_CM_K8 * A_SG_K);
          const uint sgk = sgij8 / (CM_M * A_CM_K8);
          const uint zkm8 = sgij8 % (CM_M * A_CM_K8);
          const uint zm = zkm8 / A_CM_K8;
          const uint zk8 = k8 + sgk * A_CM_K8 + zkm8 % A_CM_K8;
          const uint zrs = zk8 / A_IN_CH8;
          const uint zc8 = zk8 % A_IN_CH8;
          const uint zr = zrs / KERNEL_X;
          const uint zs = zrs % KERNEL_X;
          const uint zx = wgxy.x + zs + zm - PADDING_X;
          const uint zy = wgxy.y + sgmi * SG_M + sgm + zr - PADDING_Y;
          if (zx < pc.IN_W && zy < pc.IN_H && zc8 < A_IN_CH8) {
            const uint o = zy * (pc.IN_W * A_IN_CH8) + zx * A_IN_CH8 + zc8;
            v = a_input_tensor_blob[o];
          }
        }
        prefetch_feature[q] = v;
      }
    }
  }
  #elif defined(A_IN_LAYOUT_CHWC8)
  #define FEATURE_LAYOUT_ROW_MAJOR
  #define FEATURE_LAYOUT_LINEAR
  #define FEATURE_LAYOUT_RSC
  {
    const uint k8 = subgroupBroadcastFirst(A_CM_K8 * A_SG_K * kk);
    const uint A_IN_CH8 = A_IN_CH / 8;
    for (uint q = 0; q < PREFETCH_A_IQQ; ++q) {
      uvec4 v = uvec4(0);
      const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
      if (liq8 < PREFETCH_A_SQQ) {
        const uint siq8 = sgni * PREFETCH_A_SQQ + liq8;
        const uint sgm = siq8 / (CM_M * A_CM_K8 * A_SG_K);
        const uint sgij8 = siq8 % (CM_M * A_CM_K8 * A_SG_K);
        const uint sgk = sgij8 / (CM_M * A_CM_K8);
        const uint zkm8 = sgij8 % (CM_M * A_CM_K8);
        const uint zm = zkm8 / A_CM_K8;
        const uint zk8 = k8 + sgk * A_CM_K8 + zkm8 % A_CM_K8;
        const uint zrs = zk8 / A_IN_CH8;
        const uint zc8 = zk8 % A_IN_CH8;
        const uint zr = zrs / KERNEL_X;
        const uint zs = zrs % KERNEL_X;
        const uint zx = wgxy.x + zs + zm - PADDING_X;
        const uint zy = wgxy.y + sgmi * SG_M + sgm + zr - PADDING_Y;
        if (zx < pc.IN_W && zy < pc.IN_H && zc8 < A_IN_CH8) {
          const uint o = zc8 * (pc.IN_W * pc.IN_H) + zy * (pc.IN_W) + zx;
          v = a_input_tensor_blob[o];
        }
      }
      prefetch_feature[q] = v;
    }
  }
  #else
  UNDEFINED_INPUT_LAYOUT;
  #endif
  #else
  NOT_IMPLEMENTED; // vstype != uvec4
  #endif
  #pragma end_block; // prefetch_A

  #pragma begin_block(prefetch_B);
  #if VSTYPE_SIZE == 16
  #if defined(B_IN_LAYOUT_HWC)
  {
    #define FEATURE_LAYOUT_COLUMN_MAJOR
    #define FEATURE_LAYOUT_LINEAR
    #define FEATURE_LAYOUT_RSC
    const uint k = subgroupBroadcastFirst(B_CM_K * B_SG_K * kk);
    for (uint q = 0; q < PREFETCH_B_IQQ; ++q) {
      uvec4 v = uvec4(0);
      const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
      if (liq8 < PREFETCH_B_SQQ) {
        const uint siq8 = sgni * PREFETCH_B_SQQ + liq8;
        const uint sgm = siq8 / (CM_M8 * B_CM_K * B_SG_K);
        const uint sgij8 = siq8 % (CM_M8 * B_CM_K * B_SG_K);
        const uint sgk = (sgij8 / CM_M8);
        const uint zm8 = (sgij8 % CM_M8);
        const uint zk = k + sgk;
        const uint zrs = zk / B_IN_CH;
        const uint zc = zk % B_IN_CH;
        const uint zr = zrs / KERNEL_X;
        const uint zs = zrs % KERNEL_X;
        const uint zy = wgxy.y + sgmi * SG_M + sgm + zr - PADDING_Y;
        const uint zx = wgxy.x + zs + zm8 * 8 - PADDING_X;
        if (zy < pc.IN_H && zk < KERNEL_X * KERNEL_Y * B_IN_CH) {
          const uint o = zy * (pc.IN_W * B_IN_CH) + zx * B_IN_CH + zc;
          if ((zx < pc.IN_W && zx + 8 < pc.IN_W)) {
            uint16_t a = b_input_tensor_blob[o + 0 * B_IN_CH];
            uint16_t b = b_input_tensor_blob[o + 1 * B_IN_CH];
            uint ab = packUint2x16(u16vec2(a, b));
            uint16_t c = b_input_tensor_blob[o + 2 * B_IN_CH];
            uint16_t d = b_input_tensor_blob[o + 3 * B_IN_CH];
            uint cd = packUint2x16(u16vec2(c, d));
            uint16_t e = b_input_tensor_blob[o + 4 * B_IN_CH];
            uint16_t f = b_input_tensor_blob[o + 5 * B_IN_CH];
            uint ef = packUint2x16(u16vec2(e, f));
            uint16_t g = b_input_tensor_blob[o + 6 * B_IN_CH];
            uint16_t h = b_input_tensor_blob[o + 7 * B_IN_CH];
            uint gh = packUint2x16(u16vec2(g, h));
            v = uvec4(ab, cd, ef, gh);
          } else {
            uint16_t a = (zx + 0) < pc.IN_W ? b_input_tensor_blob[o + 0 * B_IN_CH] : uint16_t(0);
            uint16_t b = (zx + 1) < pc.IN_W ? b_input_tensor_blob[o + 1 * B_IN_CH] : uint16_t(0);
            uint ab = packUint2x16(u16vec2(a, b));
            uint16_t c = (zx + 2) < pc.IN_W ? b_input_tensor_blob[o + 2 * B_IN_CH] : uint16_t(0);
            uint16_t d = (zx + 3) < pc.IN_W ? b_input_tensor_blob[o + 3 * B_IN_CH] : uint16_t(0);
            uint cd = packUint2x16(u16vec2(c, d));
            uint16_t e = (zx + 4) < pc.IN_W ? b_input_tensor_blob[o + 4 * B_IN_CH] : uint16_t(0);
            uint16_t f = (zx + 5) < pc.IN_W ? b_input_tensor_blob[o + 5 * B_IN_CH] : uint16_t(0);
            uint ef = packUint2x16(u16vec2(e, f));
            uint16_t g = (zx + 6) < pc.IN_W ? b_input_tensor_blob[o + 6 * B_IN_CH] : uint16_t(0);
            uint16_t h = (zx + 7) < pc.IN_W ? b_input_tensor_blob[o + 7 * B_IN_CH] : uint16_t(0);
            uint gh = packUint2x16(u16vec2(g, h));
            v = uvec4(ab, cd, ef, gh);
          }
        }
      }
      prefetch_feature[q] = v;
    }
  }
  #elif defined(B_IN_LAYOUT_HWC8)
  {
    #define FEATURE_LAYOUT_ROW_MAJOR
    #define FEATURE_LAYOUT_LINEAR
    #define FEATURE_LAYOUT_RSC
    {
      const uint k8 = subgroupBroadcastFirst(B_CM_K8 * B_SG_K * kk);
      const uint B_IN_CH8 = B_IN_CH / 8;
      for (uint q = 0; q < PREFETCH_B_IQQ; ++q) {
        uvec4 v = uvec4(0);
        const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
        if (liq8 < PREFETCH_B_SQQ) {
          const uint siq8 = sgni * PREFETCH_B_SQQ + liq8;
          const uint sgm = siq8 / (CM_M * B_CM_K8 * B_SG_K);
          const uint sgij8 = siq8 % (CM_M * B_CM_K8 * B_SG_K);
          const uint sgk = sgij8 / (CM_M * B_CM_K8);
          const uint zkm8 = sgij8 % (CM_M * B_CM_K8);
          const uint zm = zkm8 / B_CM_K8;
          const uint zk8 = k8 + sgk * B_CM_K8 + zkm8 % B_CM_K8;
          const uint zrs = zk8 / B_IN_CH8;
          const uint zc8 = zk8 % B_IN_CH8;
          const uint zr = zrs / KERNEL_X;
          const uint zs = zrs % KERNEL_X;
          const uint zx = wgxy.x + zs + zm - PADDING_X;
          const uint zy = wgxy.y + sgmi * SG_M + sgm + zr - PADDING_Y;
          if (zx < pc.IN_W && zy < pc.IN_H && zc8 < B_IN_CH8) {
            const uint o = zy * (pc.IN_W * B_IN_CH8) + zx * B_IN_CH8 + zc8;
            v = b_input_tensor_blob[o];
          }
        }
        prefetch_feature[q] = v;
      }
    }
  }
  #elif defined(B_IN_LAYOUT_CHWC8)
  #define FEATURE_LAYOUT_ROW_MAJOR
  #define FEATURE_LAYOUT_LINEAR
  #define FEATURE_LAYOUT_RSC
  {
    const uint k8 = subgroupBroadcastFirst(B_CM_K8 * B_SG_K * kk);
    const uint B_IN_CH8 = B_IN_CH / 8;
    for (uint q = 0; q < PREFETCH_B_IQQ; ++q) {
      uvec4 v = uvec4(0);
      const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
      if (liq8 < PREFETCH_B_SQQ) {
        const uint siq8 = sgni * PREFETCH_B_SQQ + liq8;
        const uint sgm = siq8 / (CM_M * B_CM_K8 * B_SG_K);
        const uint sgij8 = siq8 % (CM_M * B_CM_K8 * B_SG_K);
        const uint sgk = sgij8 / (CM_M * B_CM_K8);
        const uint zkm8 = sgij8 % (CM_M * B_CM_K8);
        const uint zm = zkm8 / B_CM_K8;
        const uint zk8 = k8 + sgk * B_CM_K8 + zkm8 % B_CM_K8;
        const uint zrs = zk8 / B_IN_CH8;
        const uint zc8 = zk8 % B_IN_CH8;
        const uint zr = zrs / KERNEL_X;
        const uint zs = zrs % KERNEL_X;
        const uint zx = wgxy.x + zs + zm - PADDING_X;
        const uint zy = wgxy.y + sgmi * SG_M + sgm + zr - PADDING_Y;
        if (zx < pc.IN_W && zy < pc.IN_H && zc8 < B_IN_CH8) {
          const uint o = zc8 * (pc.IN_W * pc.IN_H) + zy * (pc.IN_W) + zx;
          v = b_input_tensor_blob[o];
        }
      }
      prefetch_feature[q] = v;
    }
  }
  #else
  UNDEFINED_INPUT_LAYOUT;
  #endif
  #else
  NOT_IMPLEMENTED; // vstype != uvec4
  #endif
  #pragma end_block; // prefetch_A

  #pragma begin_block(prefetch_A_filter)
  #if VSTYPE_SIZE == 16
  #ifdef FEATURE_LAYOUT_RSC
  #if defined(A_FILTER_LAYOUT_KRSCK8) || defined(A_FILTER_LAYOUT_KRSCK16)
  #define FILTER_LAYOUT_LINEAR
  #define FILTER_LAYOUT_ROW_MAJOR
  const uint k = subgroupBroadcastFirst(A_CM_K * A_SG_K * kk);
  for (uint q = 0; q < PREFETCH_A_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_A_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_A_FILTER_SQQ + liq8;
      const uint sgn = siq8 / (CM_N8 * A_CM_K * A_SG_K);
      const uint sgij8 = siq8 % (CM_N8 * A_CM_K * A_SG_K);
      const uint zk = k + sgij8 / CM_N8;
      const uint zn8 = sgij8 % CM_N8;
      if (zk < A_RSC) {
        const uint o = ((wgni) / CM_N + sgn + sgni * SG_N) * (A_RSC * CM_N8) + zk * CM_N8 + zn8;
        v = a_filter_blob[o];
      }
    }
    prefetch_filter[q] = v;
  }
  #elif defined(A_FILTER_LAYOUT_RSCKC8) || defined(A_FILTER_LAYOUT_RSCKC16)
  #define FILTER_LAYOUT_LINEAR
  #define FILTER_LAYOUT_COLUMN_MAJOR
  for (uint q = 0; q < PREFETCH_A_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_A_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_A_FILTER_SQQ + liq8;
      const uint sgk = siq8 / (A_CM_K8 * CM_N * SG_N);
      const uint sgij8 = siq8 % (A_CM_K8 * CM_N * SG_N);
      const uint zn = wgni + sgni * (SG_N * CM_N) + sgij8 / A_CM_K8;
      const uint zk8 = sgij8 % A_CM_K8;
      if (zn < OUT_CH) {
        const uint o = (kk * A_SG_K + sgk) * (OUT_CH * A_CM_K8) + zn * A_CM_K8 + zk8;
        v = a_filter_blob[o];
      }
    }
    prefetch_filter[q] = v;
  }
  #elif defined(A_FILTER_LAYOUT_RSCK)
  #define FILTER_LAYOUT_LINEAR
  #define FILTER_LAYOUT_COLUMN_MAJOR
  const uint k8 = subgroupBroadcastFirst(A_CM_K8 * A_SG_K * kk);
  for (uint q = 0; q < PREFETCH_A_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_A_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_A_FILTER_SQQ + liq8;
      const uint sgk = siq8 / (A_CM_K8 * CM_N * SG_N);
      const uint sgij8 = siq8 % (A_CM_K8 * CM_N * SG_N);
      const uint sgn = sgij8 / (A_CM_K8 * CM_N);
      const uint zkn8 = sgij8 % (A_CM_K8 * CM_N);
      const uint zn = wgni + sgni * (CM_N * SG_N) + sgn * CM_N + (zkn8 / A_CM_K8);
      const uint zk8 = k8 + sgk * A_CM_K8 + zkn8 % A_CM_K8;
      const uint zk = zk8 * 8;
      if (zn < OUT_CH) {
        const uint o = zk * OUT_CH + zn;
        if (zk + 8 < A_RSC) {
          const uint16_t a = a_filter_blob[o + 0 * OUT_CH];
          const uint16_t b = a_filter_blob[o + 1 * OUT_CH];
          v.x = packUint2x16(u16vec2(a, b));
          const uint16_t c = a_filter_blob[o + 2 * OUT_CH];
          const uint16_t d = a_filter_blob[o + 3 * OUT_CH];
          v.y = packUint2x16(u16vec2(c, d));
          const uint16_t e = a_filter_blob[o + 4 * OUT_CH];
          const uint16_t f = a_filter_blob[o + 5 * OUT_CH];
          v.z = packUint2x16(u16vec2(e, f));
          const uint16_t g = a_filter_blob[o + 6 * OUT_CH];
          const uint16_t h = a_filter_blob[o + 7 * OUT_CH];
          v.w = packUint2x16(u16vec2(g, h));
        } else {
          const uint16_t a = ((zk + 0) < A_RSC) ? a_filter_blob[o + 0 * OUT_CH] : uint16_t(0);
          const uint16_t b = ((zk + 1) < A_RSC) ? a_filter_blob[o + 1 * OUT_CH] : uint16_t(0);
          v.x = packUint2x16(u16vec2(a, b));
          const uint16_t c = ((zk + 2) < A_RSC) ? a_filter_blob[o + 2 * OUT_CH] : uint16_t(0);
          const uint16_t d = ((zk + 3) < A_RSC) ? a_filter_blob[o + 3 * OUT_CH] : uint16_t(0);
          v.y = packUint2x16(u16vec2(c, d));
          const uint16_t e = ((zk + 4) < A_RSC) ? a_filter_blob[o + 4 * OUT_CH] : uint16_t(0);
          const uint16_t f = ((zk + 5) < A_RSC) ? a_filter_blob[o + 5 * OUT_CH] : uint16_t(0);
          v.z = packUint2x16(u16vec2(e, f));
          const uint16_t g = ((zk + 6) < A_RSC) ? a_filter_blob[o + 6 * OUT_CH] : uint16_t(0);
          const uint16_t h = ((zk + 7) < A_RSC) ? a_filter_blob[o + 7 * OUT_CH] : uint16_t(0);
          v.w = packUint2x16(u16vec2(g, h));
        }
      }
    }
    prefetch_filter[q] = v;
  }
  #else
  UNDEFINED_FILTER_LAYOUT; // Undefined Filter layout.
  #endif
  #else
  UNDEFINED_A_LAYOUT; // Undefined A layout
  #endif
  #else
  NOT_IMPLEMENTED; // vstype != null
  #endif
  #pragma end_block; // prefetch_A_filter

  #pragma begin_block(prefetch_B_filter)
  #if VSTYPE_SIZE == 16
  #ifdef FEATURE_LAYOUT_RSC
  #if defined(B_FILTER_LAYOUT_KRSCK8) || defined(B_FILTER_LAYOUT_KRSCK16)
  #define FILTER_LAYOUT_LINEAR
  #define FILTER_LAYOUT_ROW_MAJOR
  const uint k = subgroupBroadcastFirst(B_CM_K * B_SG_K * kk);
  for (uint q = 0; q < PREFETCH_B_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_B_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_B_FILTER_SQQ + liq8;
      const uint sgn = siq8 / (CM_N8 * B_CM_K * B_SG_K);
      const uint sgij8 = siq8 % (CM_N8 * B_CM_K * B_SG_K);
      const uint zk = k + sgij8 / CM_N8;
      const uint zn8 = sgij8 % CM_N8;
      if (zk < B_RSC) {
        const uint o = ((wgni) / CM_N + sgn + sgni * SG_N) * (B_RSC * CM_N8) + zk * CM_N8 + zn8;
        v = b_filter_blob[o];
      }
    }
    prefetch_filter[q] = v;
  }
  #elif defined(B_FILTER_LAYOUT_RSCKC8) || defined(B_FILTER_LAYOUT_RSCKC16)
  #define FILTER_LAYOUT_LINEAR
  #define FILTER_LAYOUT_COLUMN_MAJOR
  for (uint q = 0; q < PREFETCH_B_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_B_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_B_FILTER_SQQ + liq8;
      const uint sgk = siq8 / (B_CM_K8 * CM_N * SG_N);
      const uint sgij8 = siq8 % (B_CM_K8 * CM_N * SG_N);
      const uint zn = wgni + sgni * (SG_N * CM_N) + sgij8 / B_CM_K8;
      const uint zk8 = sgij8 % B_CM_K8;
      if (zn < OUT_CH) {
        const uint o = (kk * B_SG_K + sgk) * (OUT_CH * B_CM_K8) + zn * B_CM_K8 + zk8;
        v = b_filter_blob[o];
      }
    }
    prefetch_filter[q] = v;
  }
  #elif defined(B_FILTER_LAYOUT_RSCK)
  #define FILTER_LAYOUT_LINEAR
  #define FILTER_LAYOUT_COLUMN_MAJOR
  const uint k8 = subgroupBroadcastFirst(B_CM_K8 * B_SG_K * kk);
  for (uint q = 0; q < PREFETCH_B_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_B_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_B_FILTER_SQQ + liq8;
      const uint sgk = siq8 / (B_CM_K8 * CM_N * SG_N);
      const uint sgij8 = siq8 % (B_CM_K8 * CM_N * SG_N);
      const uint sgn = sgij8 / (B_CM_K8 * CM_N);
      const uint zkn8 = sgij8 % (B_CM_K8 * CM_N);
      const uint zn = wgni + sgni * (CM_N * SG_N) + sgn * CM_N + (zkn8 / B_CM_K8);
      const uint zk8 = k8 + sgk * B_CM_K8 + zkn8 % B_CM_K8;
      const uint zk = zk8 * 8;
      if (zn < OUT_CH) {
        const uint o = zk * OUT_CH + zn;
        if (zk + 8 < B_RSC) {
          const uint16_t a = b_filter_blob[o + 0 * OUT_CH];
          const uint16_t b = b_filter_blob[o + 1 * OUT_CH];
          v.x = packUint2x16(u16vec2(a, b));
          const uint16_t c = b_filter_blob[o + 2 * OUT_CH];
          const uint16_t d = b_filter_blob[o + 3 * OUT_CH];
          v.y = packUint2x16(u16vec2(c, d));
          const uint16_t e = b_filter_blob[o + 4 * OUT_CH];
          const uint16_t f = b_filter_blob[o + 5 * OUT_CH];
          v.z = packUint2x16(u16vec2(e, f));
          const uint16_t g = b_filter_blob[o + 6 * OUT_CH];
          const uint16_t h = b_filter_blob[o + 7 * OUT_CH];
          v.w = packUint2x16(u16vec2(g, h));
        } else {
          const uint16_t a = ((zk + 0) < B_RSC) ? b_filter_blob[o + 0 * OUT_CH] : uint16_t(0);
          const uint16_t b = ((zk + 1) < B_RSC) ? b_filter_blob[o + 1 * OUT_CH] : uint16_t(0);
          v.x = packUint2x16(u16vec2(a, b));
          const uint16_t c = ((zk + 2) < B_RSC) ? b_filter_blob[o + 2 * OUT_CH] : uint16_t(0);
          const uint16_t d = ((zk + 3) < B_RSC) ? b_filter_blob[o + 3 * OUT_CH] : uint16_t(0);
          v.y = packUint2x16(u16vec2(c, d));
          const uint16_t e = ((zk + 4) < B_RSC) ? b_filter_blob[o + 4 * OUT_CH] : uint16_t(0);
          const uint16_t f = ((zk + 5) < B_RSC) ? b_filter_blob[o + 5 * OUT_CH] : uint16_t(0);
          v.z = packUint2x16(u16vec2(e, f));
          const uint16_t g = ((zk + 6) < B_RSC) ? b_filter_blob[o + 6 * OUT_CH] : uint16_t(0);
          const uint16_t h = ((zk + 7) < B_RSC) ? b_filter_blob[o + 7 * OUT_CH] : uint16_t(0);
          v.w = packUint2x16(u16vec2(g, h));
        }
      }
    }
    prefetch_filter[q] = v;
  }
  #else
  UNDEFINED_FILTER_LAYOUT; // Undefined Filter layout.
  #endif
  #else
  UNDEFINED_A_LAYOUT; // Undefined A layout
  #endif
  #else
  NOT_IMPLEMENTED; // vstype != null
  #endif
  #pragma end_block; // prefetch_B_filter

  #pragma begin_block(copy_prefetch_A_feature);
  #if VSTYPE_SIZE == 16
  #ifdef FEATURE_LAYOUT_LINEAR
  {
    for (uint q = 0; q < PREFETCH_A_IQQ; ++q) {
      const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
      if (liq8 < PREFETCH_A_SQQ) {
        const uint siq8 = sgni * PREFETCH_A_SQQ + liq8;
        sh_buf[SH_A_OFFSET + sgmi * PREFETCH_A_QQ + siq8] = prefetch_feature[q];
      }
    }
  }
  #else
  NOT_IMPLEMENTED;
  #endif
  #else
  NOT_DESIGNED_FOR_THIS;
  #endif
  #pragma end_block; // copy_prefetch_A

  #pragma begin_block(copy_prefetch_B_feature);
  #if VSTYPE_SIZE == 16
  #ifdef FEATURE_LAYOUT_LINEAR
  {
    for (uint q = 0; q < PREFETCH_B_IQQ; ++q) {
      const uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
      if (liq8 < PREFETCH_B_SQQ) {
        const uint siq8 = sgni * PREFETCH_B_SQQ + liq8;
        sh_buf[SH_B_OFFSET + sgmi * PREFETCH_B_QQ + siq8] = prefetch_feature[q];
      }
    }
  }
  #else
  NOT_IMPLEMENTED;
  #endif
  #else
  NOT_DESIGNED_FOR_THIS;
  #endif
  #pragma end_block; // copy_prefetch_A

  #pragma begin_block(copy_prefetch_A_filter);
  #if VSTYPE_SIZE == 16
  #ifdef FILTER_LAYOUT_LINEAR
  for (uint q = 0; q < PREFETCH_A_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_A_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_A_FILTER_SQQ + liq8;
      sh_buf[SH_A_FILTER_OFFSET + sgni * PREFETCH_A_FILTER_QQ + siq8] = prefetch_filter[q];
    }
  }
  #else
  NOT_IMPLEMENTED;
  #endif
  #else
  NOT_DESIGNED_FOR_THIS; // vstype != uvec4.
  #endif
  #pragma end_block; // copy_prefetch_filter

  #pragma begin_block(copy_prefetch_B_filter);
  #if VSTYPE_SIZE == 16
  #ifdef FILTER_LAYOUT_LINEAR
  for (uint q = 0; q < PREFETCH_B_FILTER_IQQ; ++q) {
    uint liq8 = SG_SIZE * q + gl_SubgroupInvocationID;
    uvec4 v = uvec4(0);
    if (liq8 < PREFETCH_B_FILTER_SQQ) {
      const uint siq8 = sgmi * PREFETCH_B_FILTER_SQQ + liq8;
      sh_buf[SH_B_FILTER_OFFSET + sgni * PREFETCH_B_FILTER_QQ + siq8] = prefetch_filter[q];
    }
  }
  #else
  NOT_IMPLEMENTED;
  #endif
  #else
  NOT_DESIGNED_FOR_THIS; // vstype != uvec4.
  #endif
  #pragma end_block; // copy_prefetch_filter

  #pragma begin_block(gemm_A);
  {
    coopmat < atype, gl_ScopeSubgroup, CM_M, A_CM_K, gl_MatrixUseA > A[SG_M];
    coopmat < atype, gl_ScopeSubgroup, A_CM_K, CM_N, gl_MatrixUseB > B[SG_N];
    for (uint zk = 0; zk < A_SG_K; ++zk) {
      for (uint zm = 0; zm < SG_M; ++zm) {
        #ifdef FEATURE_LAYOUT_COLUMN_MAJOR
        {
          const uint CM_M_VS = CM_M / (VSTYPE_SIZE / ATYPE_SIZE);
          coopMatLoad(A[zm],
            sh_buf,
            SH_A_OFFSET + sgmi * PREFETCH_A_QQ + (zm * A_SG_K + zk) * (CM_M_VS * A_CM_K),
            CM_M_VS,
            gl_CooperativeMatrixLayoutColumnMajor);
        }
        #elif defined(FEATURE_LAYOUT_ROW_MAJOR)
        {
          const uint A_CM_K_VS = A_CM_K / (VSTYPE_SIZE / ATYPE_SIZE);
          coopMatLoad(A[zm],
            sh_buf,
            SH_A_OFFSET + sgmi * PREFETCH_A_QQ + (zm * A_SG_K + zk) * (CM_M * A_CM_K_VS),
            A_CM_K_VS,
            gl_CooperativeMatrixLayoutRowMajor);
        }
        #else
        UNDEFINED_FEATURE_LAYOUT; // Unsupported prefetch_A layout.
        #endif
      }
      for (uint zn = 0; zn < SG_N; zn++) {
        #ifdef FILTER_LAYOUT_COLUMN_MAJOR
        {
          const uint A_CM_K_VS = A_CM_K / (VSTYPE_SIZE / ATYPE_SIZE);
          coopMatLoad(B[zn],
            sh_buf,
            SH_A_FILTER_OFFSET + sgni * PREFETCH_A_FILTER_QQ + (zk * SG_N + zn) * (A_CM_K_VS * CM_N),
            A_CM_K_VS,
            gl_CooperativeMatrixLayoutColumnMajor);
        }
        #elif defined(FILTER_LAYOUT_ROW_MAJOR)
        const uint CM_N_VS = CM_N / (VSTYPE_SIZE / ATYPE_SIZE);
        coopMatLoad(B[zn],
          sh_buf,
          SH_A_FILTER_OFFSET + sgni * PREFETCH_A_FILTER_QQ + (zn * A_SG_K + zk) * (CM_N_VS * A_CM_K),
          CM_N_VS,
          gl_CooperativeMatrixLayoutRowMajor);
        #else
        UNDEFIND_PREFETCH_B_LAYOUT;
        #endif
      }
      for (uint zn = 0; zn < SG_N; zn++) {
        for (uint zm = 0; zm < SG_M; zm++) {
          sum[zn][zm] = coopMatMulAdd(A[zm], B[zn], sum[zn][zm]);
        }
      }
    }
  }
  #pragma end_block; // gemm

  #pragma begin_block(gemm_B);
  {
    coopmat < atype, gl_ScopeSubgroup, CM_M, B_CM_K, gl_MatrixUseA > A[SG_M];
    coopmat < atype, gl_ScopeSubgroup, B_CM_K, CM_N, gl_MatrixUseB > B[SG_N];
    for (uint zk = 0; zk < B_SG_K; ++zk) {
      for (uint zm = 0; zm < SG_M; ++zm) {
        #ifdef FEATURE_LAYOUT_COLUMN_MAJOR
        {
          const uint CM_M_VS = CM_M / (VSTYPE_SIZE / ATYPE_SIZE);
          coopMatLoad(A[zm],
            sh_buf,
            SH_B_OFFSET + sgmi * PREFETCH_B_QQ + (zm * B_SG_K + zk) * (CM_M_VS * B_CM_K),
            CM_M_VS,
            gl_CooperativeMatrixLayoutColumnMajor);
        }
        #elif defined(FEATURE_LAYOUT_ROW_MAJOR)
        {
          const uint B_CM_K_VS = B_CM_K / (VSTYPE_SIZE / ATYPE_SIZE);
          coopMatLoad(A[zm],
            sh_buf,
            SH_B_OFFSET + sgmi * PREFETCH_B_QQ + (zm * B_SG_K + zk) * (CM_M * B_CM_K_VS),
            B_CM_K_VS,
            gl_CooperativeMatrixLayoutRowMajor);
        }
        #else
        UNDEFINED_FEATURE_LAYOUT; // Unsupported prefetch_A layout.
        #endif
      }
      for (uint zn = 0; zn < SG_N; zn++) {
        #ifdef FILTER_LAYOUT_COLUMN_MAJOR
        {
          const uint B_CM_K_VS = B_CM_K / (VSTYPE_SIZE / ATYPE_SIZE);
          coopMatLoad(B[zn],
            sh_buf,
            SH_B_FILTER_OFFSET + sgni * PREFETCH_B_FILTER_QQ + (zk * SG_N + zn) * (B_CM_K_VS * CM_N),
            B_CM_K_VS,
            gl_CooperativeMatrixLayoutColumnMajor);
        }
        #elif defined(FILTER_LAYOUT_ROW_MAJOR)
        const uint CM_N_VS = CM_N / (VSTYPE_SIZE / ATYPE_SIZE);
        coopMatLoad(B[zn],
          sh_buf,
          SH_B_FILTER_OFFSET + sgni * PREFETCH_B_FILTER_QQ + (zn * B_SG_K + zk) * (CM_N_VS * B_CM_K),
          CM_N_VS,
          gl_CooperativeMatrixLayoutRowMajor);
        #else
        UNDEFIND_PREFETCH_B_LAYOUT;
        #endif
      }
      for (uint zn = 0; zn < SG_N; zn++) {
        for (uint zm = 0; zm < SG_M; zm++) {
          sum[zn][zm] = coopMatMulAdd(A[zm], B[zn], sum[zn][zm]);
        }
      }
    }
  }
  #pragma end_block; // gemm

  #pragma begin_block(activation)
  #if defined(ACTIVATION_NONE)
  #elif defined(ACTIVATION_ReLU)
  for (uint zn = 0; zn < SG_N; zn++) {
    for (uint zm = 0; zm < SG_M; zm++) {
      for (uint i = 0; i < sum[zn][zm].length(); ++i) {
        sum[zn][zm][i] = max(
            atype(0.0f),
            sum[zn][zm][i]);
      }
    }
  }
  #else
  UNDEFINED_ACTIVATION; // activation not one of {None, ReLU}
  #endif
  #pragma end_block; // activation

  #pragma begin_block(writeback);
  #if VSTYPE_SIZE == 16
  #if defined(OUT_LAYOUT_HWC)
  #if ATYPE_SIZE == 2
  for (uint zn = 0; zn < SG_N; zn++) {
    for (uint zm = 0; zm < SG_M; zm++) {
      const uint CM_M_VS = CM_M / (VSTYPE_SIZE / ATYPE_SIZE);
      coopMatStore(sum[zn][zm],
        sh_buf,
        SH_OUT_OFFSET + sgi * SH_OUT_SG_SIZE8 + (zm * SG_N + zn) * (CM_M_VS * CM_N),
        CM_M_VS,
        gl_CooperativeMatrixLayoutColumnMajor);
    }
  }
  subgroupMemoryBarrierShared();
  subgroupBarrier();
  {
    for (uint sgm = 0; sgm < SG_M; ++sgm) {
      const uint o = SH_OUT_OFFSET + sgi * SH_OUT_SG_SIZE8 + sgm * (SG_N * CM_M * CM_N / (VSTYPE_SIZE / ATYPE_SIZE));
      const uint y = wgxy.y + sgmi * SG_M + sgm;
      if (y >= pc.IN_H) {
        break;
      }
      const uint gy = y * pc.IN_W * OUT_CH;
      for (uint sgij = gl_SubgroupInvocationID; sgij < (CM_M8 * CM_N * SG_N); sgij += SG_SIZE) {
        const uint sgn = sgij / CM_M8;
        const uint zn = wgni + sgni * (CM_N * SG_N) + sgn;
        const uint zm = (sgij % ((CM_M * ATYPE_SIZE) / VSTYPE_SIZE)) * (VSTYPE_SIZE / ATYPE_SIZE);
        const uint zx = wgxy.x + zm;
        if (int(zn) < OUT_CH) {
          uint go = gy + zx * OUT_CH + zn;
          uvec4 v = sh_buf[o + sgij];
          u16vec2 vx = unpackUint2x16(v.x);
          u16vec2 vy = unpackUint2x16(v.y);
          u16vec2 vz = unpackUint2x16(v.z);
          u16vec2 vw = unpackUint2x16(v.w);
          if (int(zx + 8) <= pc.IN_W) {
            output_tensor_blob[go] = vx.x;
            output_tensor_blob[go + OUT_CH] = vx.y;
            output_tensor_blob[go + 2 * OUT_CH] = vy.x;
            output_tensor_blob[go + 3 * OUT_CH] = vy.y;
            output_tensor_blob[go + 4 * OUT_CH] = vz.x;
            output_tensor_blob[go + 5 * OUT_CH] = vz.y;
            output_tensor_blob[go + 6 * OUT_CH] = vw.x;
            output_tensor_blob[go + 7 * OUT_CH] = vw.y;
          } else {
            if (zx < pc.IN_W) {
              output_tensor_blob[go] = vx.x;
            }
            if ((zx + 1) < pc.IN_W) {
              output_tensor_blob[go + OUT_CH] = vx.y;
            }
            if ((zx + 2) < pc.IN_W) {
              output_tensor_blob[go + 2 * OUT_CH] = vy.x;
            }
            if ((zx + 3) < pc.IN_W) {
              output_tensor_blob[go + 3 * OUT_CH] = vy.y;
            }
            if ((zx + 4) < pc.IN_W) {
              output_tensor_blob[go + 4 * OUT_CH] = vz.x;
            }
            if ((zx + 5) < pc.IN_W) {
              output_tensor_blob[go + 5 * OUT_CH] = vz.y;
            }
            if ((zx + 6) < pc.IN_W) {
              output_tensor_blob[go + 6 * OUT_CH] = vw.x;
            }
            if ((zx + 7) < pc.IN_W) {
              output_tensor_blob[go + 7 * OUT_CH] = vw.y;
            }
          }
        }
      }
    }
  }
  #else
  NOT_SUPPORTED; // atype != float16_t
  #endif
  #elif defined(OUT_LAYOUT_HWC8)
  #if ATYPE_SIZE == 2
  for (uint zn = 0; zn < SG_N; zn++) {
    for (uint zm = 0; zm < SG_M; zm++) {
      coopMatStore(sum[zn][zm],
        sh_buf,
        SH_OUT_OFFSET + sgi * SH_OUT_SG_SIZE8 + (zm * SG_N + zn) * (CM_N * CM_M) / (VSTYPE_SIZE / ATYPE_SIZE),
        CM_N / (VSTYPE_SIZE / ATYPE_SIZE),
        gl_CooperativeMatrixLayoutRowMajor);
    }
  }
  subgroupMemoryBarrierShared();
  subgroupBarrier();
  const uint CM_M8 = CM_M / (VSTYPE_SIZE / ATYPE_SIZE);
  const uint CM_N8 = CM_N / (VSTYPE_SIZE / ATYPE_SIZE);
  const uint OUT_CH8 = OUT_CH / (VSTYPE_SIZE / ATYPE_SIZE);
  for (uint sgm = 0; sgm < SG_M; ++sgm) {
    const uint QQ = (CM_N8 * CM_M * SG_N);
    const uint o = SH_OUT_OFFSET + sgi * SH_OUT_SG_SIZE8 + sgm * QQ;
    const uint y = wgxy.y + sgmi * SG_M + sgm;
    if (y >= pc.IN_H) {
      continue;
    }
    const uint gy8 = y * pc.IN_W * OUT_CH8;
    for (uint q = gl_SubgroupInvocationID; q < QQ; q += SG_SIZE) {
      const uint sgzn = q / (CM_N8 * CM_M);
      const uint sgij = q % (CM_N8 * CM_M);
      const uint zx = wgxy.x + sgij / CM_N8;
      const uint wgni8 = wgni / (VSTYPE_SIZE / ATYPE_SIZE);
      const uint zn8 = wgni8 + sgni * (CM_N8 * SG_N) + sgzn * CM_N8 + (sgij % CM_N8);
      if (zn8 < OUT_CH8 && zx < pc.IN_W) {
        const uvec4 v = sh_buf[o + q];
        output_tensor_blob[gy8 + zx * OUT_CH8 + zn8] = v;
      }
    }
  }
  #else
  NOT_IMPLEMENTED;
  #endif

  #elif defined(OUT_LAYOUT_CHWC8)
  #if ATYPE_SIZE == 2
  for (uint zn = 0; zn < SG_N; zn++) {
    for (uint zm = 0; zm < SG_M; zm++) {
      coopMatStore(sum[zn][zm],
        sh_buf,
        SH_OUT_OFFSET + sgi * SH_OUT_SG_SIZE8 + (zn * SG_M + zm) * (CM_N * CM_M) / (VSTYPE_SIZE / ATYPE_SIZE),
        CM_N / (VSTYPE_SIZE / ATYPE_SIZE),
        gl_CooperativeMatrixLayoutRowMajor);
    }
  }
  // force shared memory writeback.
  subgroupMemoryBarrierShared();
  subgroupBarrier(); // <- basically a noop.

  const uint CM_N8 = CM_N / (OSTYPE_SIZE / ATYPE_SIZE);
  const uint CM_M8 = CM_M / (OSTYPE_SIZE / ATYPE_SIZE);
  const uint OUT_CH8 = OUT_CH / (OSTYPE_SIZE / ATYPE_SIZE);

  for (uint sgzn = 0; sgzn < SG_N; sgzn++) {
    const uint QQ = CM_N8 * CM_M * SG_M;
    const uint o = SH_OUT_OFFSET + sgi * SH_OUT_SG_SIZE8 + sgzn * QQ;

    const uint wgni8 = wgni / (OSTYPE_SIZE / ATYPE_SIZE);
    // global channel offset (uvec4)
    const uint sgn = wgni8 + (CM_N8 * SG_N) * sgni + sgzn * CM_N8;
    const uint IN_HW = pc.IN_W * pc.IN_H;

    for (uint q = gl_SubgroupInvocationID; q < QQ; q += SG_SIZE) {
      const uint sgzm = q / (CM_N8 * CM_M);
      const uint sgij = q % (CM_N8 * CM_M);
      const uint sgx = wgxy.x + sgij / CM_N8;
      const uint zn8 = (sgij % CM_N8);
      const uint n8 = zn8 + sgn;

      const uint y = wgxy.y + sgmi * SG_M + sgzm;

      if (n8 < OUT_CH8 && y < pc.IN_H && sgx < pc.IN_W) {
        uvec4 v = sh_buf[o + q];
        output_tensor_blob[n8 * IN_HW + y * pc.IN_W + sgx] = v;
      }
    }
  }
  #else
  NOT_IMPLEMENTED; // atype != float16_t
  #endif
  #else
  UNDEFINED_OUT_LAYOUT;
  #endif
  #else
  NOT_IMPLEMENTED; // vstype != uvec4
  #endif
  #pragma end_block(writeback);

  // =============== MAIN ==============
  coopmat < atype, gl_ScopeSubgroup, CM_M, CM_N, gl_MatrixUseAccumulator > sum[SG_N][SG_M];
  #pragma inline_block(load_bias);
  { // A implicit GEMM
    uint kk = 0;
    const uint KK = ((KERNEL_X * KERNEL_Y * A_IN_CH) + (A_CM_K * A_SG_K) - 1) / (A_CM_K * A_SG_K);
    vstype prefetch_feature[PREFETCH_A_IQQ];
    vstype prefetch_filter[PREFETCH_A_FILTER_IQQ];
    #ifdef A_ASYNC_READ
    #pragma inline_block(prefetch_A);
    #pragma inline_block(prefetch_A_filter);
    kk += 1;
    for (; kk < KK; ++kk) {
      #pragma inline_block(copy_prefetch_A_feature);
      #pragma inline_block(copy_prefetch_A_filter);
      barrier();
      #pragma inline_block(prefetch_A);
      #pragma inline_block(prefetch_A_filter);
      #pragma inline_block(gemm_A);
      barrier();
    }
    #pragma inline_block(copy_prefetch_A_feature);
    #pragma inline_block(copy_prefetch_A_filter);
    barrier();
    #pragma inline_block(gemm_A);

    #else
    for (; kk < KK; ++kk) {
      #pragma inline_block(prefetch_A);
      #pragma inline_block(prefetch_A_filter);
      #pragma inline_block(copy_prefetch_A_feature);
      #pragma inline_block(copy_prefetch_A_filter);
      barrier();
      #pragma inline_block(gemm_A);
      barrier();
    }
    #endif
  }
  #undef FEATURE_LAYOUT_ROW_MAJOR
  #undef FEATURE_LAYOUT_COLUMN_MAJOR
  #undef FEATURE_LAYOUT_LINEAR
  #undef FEATURE_LAYOUT_RSC
  #undef FILTER_LAYOUT_LINEAR
  #undef FILTER_LAYOUT_ROW_MAJOR
  #undef FILTER_LAYOUT_COLUMN_MAJOR
  {
    uint kk = 0;
    const uint KK = ((KERNEL_X * KERNEL_Y * B_IN_CH) + (B_CM_K * B_SG_K) - 1) / (B_CM_K * B_SG_K);
    vstype prefetch_feature[PREFETCH_B_IQQ];
    vstype prefetch_filter[PREFETCH_B_FILTER_IQQ];
    #ifdef B_ASYNC_READ
    #pragma inline_block(prefetch_B);
    #pragma inline_block(prefetch_B_filter);
    kk += 1;
    for (; kk < KK; ++kk) {
      #pragma inline_block(copy_prefetch_B_feature);
      #pragma inline_block(copy_prefetch_B_filter);
      barrier();
      #pragma inline_block(prefetch_B);
      #pragma inline_block(prefetch_B_filter);
      #pragma inline_block(gemm_B);
      barrier();
    }
    #pragma inline_block(copy_prefetch_B_feature);
    #pragma inline_block(copy_prefetch_B_filter);
    barrier();
    #pragma inline_block(gemm_B);

    #else
    for (; kk < KK; ++kk) {
      #pragma inline_block(prefetch_B);
      #pragma inline_block(prefetch_B_filter);
      #pragma inline_block(copy_prefetch_B_feature);
      #pragma inline_block(copy_prefetch_B_filter);
      barrier();
      #pragma inline_block(gemm_B);
      barrier();
    }
    #endif
  }

  // // === Writeback ===
  #pragma inline_block(activation);
  #pragma inline_block(writeback);
}
